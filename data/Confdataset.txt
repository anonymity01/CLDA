0	social_networks probabilistic_model	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaToward Optimal Vaccination Strategies for Probabilistic ModelsZeinab AbbassiColumbia UniversityHoda HeidariSharif University of Technologyzeinab@cs.columbia.eduheidari@ce.sharif.eduCategories and Subject DescriptorsH.2.8 [Information Systems]: Applications--Data Mining2.MODELGeneral TermsAlgorithms, Economics, Performance, Human FactorsKeywordsVaccination, Social Networks, Probabilistic Model, Virus Propagation, SIR1. INTRODUCTIONEpidemic outbreaks such as the recent H1N1 influenza show how susceptible large communities are toward the spread of such outbreaks. The occurrence of a widespread disease transmission raises the question of vaccination strategies that are appropriate and close to optimal. The seemingly different problem of viruses disseminating through email networks, shares a common structure with disease epidemics. While it is not possible to vaccinate every individual during a virus outbreak, due to economic and logistical constraints, fortunately, we can leverage the structure and properties of face-to-face social networks to identify individuals whose vaccination would result in a lower number of infected people. The models that have been studied so far [3, 4] assume that once an individual is infected all its adjacent individuals would be infected with probability 1. However, this assumption is not realistic. In reality, if an individual is infected by a virus, the neighboring individuals would get infected with some probability (depending on the type of the disease and the contact). This modification to the model makes the problem more challenging as the simple version is already NP-complete [3]. Here we consider the following epidemiological model computationally: A number of individuals in the community get vaccinated which makes them immune to the disease. The disease then outbreaks and a number of nodes that are not vaccinated get infected at random. These nodes can transmit the infection to their friends with some probability. In this work we consider the optimization problem in which the number of nodes that get vaccinated is limited to k and our objective is to minimize the number of infected people overall. We design various algorithms that take into account the properties of social networks to select k nodes for vaccination in order to achieve the goal. We perform experiments on a real dataset of 34,546 vertices and 421,578 edges and assess their effectiveness and scalability.Copyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.We represent the social contact network by a graph G = (V, E), where each individual is denoted by a node in the graph. There is an edge between two nodes, if the probability of virus transmission between its endpoints is non-zero. For example, in the case of flu there is an edge between two co-workers. In this graph, let qi be the probability that node i gets infected initially (when this probability is assumed to be the same for all nodes, we denote it by q). Also, let's assume that for all i, j V , pij is the probability that j would get infected, given i is infected. The problem that we consider in this paper is defined as follows: Given a social contact network which is represented by G(V, E), the probabilities qi and pij , and a parameter k; our objective is to find k nodes to vaccinate such that the total expected number of infected nodes is minimized. In other words, let T be the set of vaccinated nodes and f (T ) be the expected number of nodes that get infected after set T is vaccinated. The goal is to find a set T (|T | = k) of nodes to vaccinate in order to minimize f (T ). Given the set T of vaccinated nodes and set S of initial infected nodes, let fS (T ) be the set of nodes that get infected through the propagation of infection according to the random process. It can be seen that f (T ) = SV (G) q(S)fS (T ) where q(S) is the probability that the set S of nodes gets initially infected and is equal to iS qi iS (1 - qi ) Evaluation Function. It can be shown that for any > 0, function f (T ) can be computed within a factor 1- in time polynomial in the size of the input and 1 [1]. Remark: f (T ) is a monotone function and is neither submodular nor supermodular. For proofs please refer to [1].3.ALGORITHMSSince the problem of vaccination is shown to be N P -hard [3], we design several heuristic algorithms to tackle it. Our algorithms have a common basic structure: for all of them, we define a measure (M ) by means of which we calculate the vaccination priority of the nodes. More precisely, we iteratively calculate M for all remaining nodes of the input network and vaccinate the node whose M is extremum, until sufficient number of nodes get vaccinated. We call this template Iterative Candidate Selection Algorithm (ICSA). In order to improve the running time of ICSA, we need to decrease the number of times it updates M . So we modify it in a way that it only updates M when cn (n = 0, 1, 2, ...) percentage of nodes have already been vaccinated. The reason that we update the measure more frequently at the beginning is that deleting a node when the graph is dense has probably more impact on the expected number of infected nodes than when the graph has become sparse. The rest of this section is devoted to the measures we have used.1WWW 2011 ­ PosterPerformance (q=0.1) Performance (q=0.026) 70 60 50 40 30 20 10 0 High Betweenness Heuristic High Degree Heuristic Greedy Heuristic Local Search 70 60 50 40 30 20 10 0 High Betweenness Heuristic High Degree Heuristic Greedy Heuristic Local SearchMarch 28­April 1, 2011, Hyderabad, IndiaPerformance (q=0.01) 70 60 50 40 30 20 10 0 High Betweenness Heuristic High Degree Heuristic Greedy Heuristic Local SearchExpected Percentage of Infected Nodes (f(T))Expected Percentage of Infected Nodes (f(T))020 40 60 80 Percentage Of Vaccinated Nodes (|T|)020 40 60 80 Percentage Of Vaccinated Nodes (|T|)Expected Percentage of Infected Nodes (f(T))020 40 60 80 Percentage Of Vaccinated Nodes (|T|)Figure 1: Performance of different heuristics on HEP-PH with p = 0.05. High Degree. An intuitive vaccination strategy is to vaccinate the individuals who have more contact with others. In order to consider the likelihood of transmission of disease as well, we define the expected degree of a node to be the sum of the probabilities (p) of its incident edges. The High Degree heuristic is the (modified) ICSA where its priority measure is set to expected degree. High Betweenness. Another centrality measure that we consider is node betweenness. Vaccinating nodes of high betweenness can prevent an epidemic to be transmitted to a large group of nodes. Again here, we take into account the likelihood that virus reaches other nodes from a node and define expected betweenness of a node to be the overall expected amount of flow that it receives. In the High Betweenness Algorithm we use expected betweenness as the measure M for the ICSA. Greedy. A natural strategy for selecting good candidate nodes is the greedy algorithm: in each step, vaccinate the node which decreases f by the most value. The greedy algorithm would be time-consuming for large networks, since at each step, one has to try all nodes u T , and compute f (T {u}) via sampling. One way to improve the running time is to compute f (T {u}) only for nodes in a priority queue that contains important nodes, like nodes with high expected degree. By manipulating the size of this queue, we can adjust accuracy and running time of the algorithm. Local Search. To improve the results of the above algorithms, one can try a natural local search, a.k.a Swap algorithm. An improving swap here means to find u T and v T so as f (T {v}\{u}) < f (T ) and substitute u with v, i.e. T = T {v}\{u}. In the local search algorithm, we perform the swap that decreases f by most value, as far as an improving swap exists. To make the local search scalable, we examine only the possible swaps between high expected degree nodes of V - T and T . percentage of expected infected nodes to less than a third by only vaccinating %16 of the nodes. The local search algorithm depicted in Figure 1 is performed on the results of the greedy algorithm, therefore it is expected to perform at least the same. The results show slight improvements, which implies that our greedy algorithm works well leaving minimal space for improvements while taking much less time than the local search operations. The above trends hold for the trial in which p = 0.005 which is not shown here due to space constraints. We also created an equivalent Erdos-Renyi graph and performed the same experiments on them. All the algorithms are less effective on random graphs as expected, but their relevant performance is the same. The running times of the algorithms are shown in Table 1. For figures and more details of the experiments please see [1]. Table 1: Running times in seconds. Degree Betweenness Greedy LocalSearch 149 22069 64676 1787035. CONCLUSION AND FUTURE DIRECTIONSIn this work we consider the problem of vaccination (over a contact network or an email network), where constraints only allow k individuals to be vaccinated. We propose a few heuristics and compare their performance and running time on a real citation network with 34, 546 vertices and 421, 578 edges. In general, we observe that by leveraging the properties of the network to select the best candidates for vaccination, we get very good results. We show that by vaccinating only a small fraction of the nodes we can decrease the spread of viruses by a lot. We also conclude that greedy is the best algorithm, both in terms of performance and running time. Interesting future directions are devising improved heuristics and possibly approximation algorithms, and also looking at the problem of minimizing overall cost when k is not given.4. EXPERIMENTAL EVALUATIONWe conduct experiments on our algorithms with the objective to asses and compare their performance and running time. For this work we have selected the HEP-PH citation dataset which was initially used in the 2003 KDD cup challenge and covers all the citations within a dataset of 34,546 papers with 421,578 edges. Citation/collaboration networks are shown to be good representations of social networks [5, 2]. If a paper p cites paper q, the graph contains a directed edge from p to q. For the purposes of our application we treat the graph as undirected [6]. The experiments are run with the following parameters: c = 2, = 0.001, p = 0.05, and 0.005 and q = 0.1, 0.01, and 5 .|V |6.[1] www.cs.columbia.edu/~zeinab/poster.pdf [2] D. Kempe, J. Kleinberg, É. Tardos, Maximizing the Spread of Influence through a Social Network, KDD'03. [3] J. Aspnes, K. Chang, A. Yampolskiy, Inoculation Strategies for Victims of Viruses and the Sum-of-Squares Partition Problem, Journal of Computer and System Sciences, 2006. [4] P. Chen, M. David, D. Kempe, Better Vaccination Strategies for Better People, EC'10. [5] M. Newman. The structure of scientific collaboration networks. Proc. Natl. Acad. Sci. 98(2001). [6] J. Gehrke, P. Ginsparg, J. M. Kleinberg. Overview of the 2003 KDD Cup. SIGKDD Explorations 2003.2
1	cache_invalidation freshness	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaTimestamp-based Cache Invalidation for Search EnginesComputer Engineering Dept. Bilkent University, TurkeySadiye AliciIsmail Sengor Altingovde ismaila@cs.bilkent.edu.trComputer Engineering Dept. Bilkent University, Turkeysadiye@cs.bilkent.edu.trrozcan@cs.bilkent.edu.trComputer Engineering Dept. Bilkent University, TurkeyRifat OzcanB. Barla Cambazoglu barla@yahoo-inc.com ABSTRACTWe propose a new mechanism to predict stale queries in the result cache of a search engine. The novelty of our approach is in the use of timestamps in staleness predictions. We show that our approach incurs very little overhead on the system while its prediction accuracy is comparable to earlier works.Yahoo! Research, Spainoulusoy@cs.bilkent.edu.trqiSEARCH NODE Document timestampsComputer Engineering Dept. Bilkent University, TurkeyÖzgür UlusoyTS(d1) TS(t1) TS(t2) ... TS(tT)TS(d2)...TS(dD)document timestamp updatesq1 R1 TS(q1) q2 R2 TS(q2) ... qC RC TS(qC)Result cache0/1Invalidation logicdocuments assigned to the nodeqi, Ri, TS(qi)miss/stale results of the node...index updatesDocument parserCategories and Subject DescriptorsH.3.3 [Information Storage and Retrieval]: Information Search and Retrievalterm timestamp updatesFigure 1: Result cache invalidation architecture. and documents. To this end, each term t in the vocabulary and each document d in the collection are associated with timestamps T S(t) and T S(d), respectively. The value of a timestamp shows the last time a term (or document) is deemed to be stale. The staleness of terms and documents are decided based on the policies given in Section 2.1. The online component is responsible for deciding on staleness of a query result. Each query q in the result cache is associated with a timestamp T S(q), showing the last time the query results are computed on the backend. A query result R in the cache can be stale due to two reasons: i) at least one of the documents in R is either deleted or updated (therefore, its rank in R is changed), or ii) at least one document that was not previously in R obtained a score high enough to enter R. The latter case is possible after the addition of a new document or update of an existing document. Our invalidation policy (Section 2.2) aims to predict whether any of these cases holds for a cached query result. In a nutshell, we compare documents' T S values to query's T S value to identify the documents that are deleted or updated after the query result was generated. To identify queries that became stale due to the second reason, we compare query terms' T S values to query's T S value to identify queries whose terms start to appear in some new documents.General TermsDesign, Performance, ExperimentationKeywordsResult cache, time-to-live, cache invalidation, freshness1. INTRODUCTIONResult caching is vital to reduce the backend query workload in search engines. In the past, eviction, admission, and prefetching issues were studied, assuming that result caches have limited capacities. Abundance of cheap storage, however, makes it attractive for search engines to cache practically all past query results. This leads to a new challenge: identifying queries with stale results in the cache [1, 2]. In this work, we propose a new mechanism, based on the use of timestamps, to predict staleness of queries. We assume an incrementally updated index (as in [1]) to which all modifications (addition, deletion, and update of documents) are continuously reflected. Our approach does not involve blind decisions (e.g., the TTL-based invalidation approach in [2]) and, in terms of computation and communication, it incurs very little overhead on the system (unlike [1]). Our experiments indicate that its accuracy in predicting stale queries is comparable to previous works.2.1Timestamp Update Policies2. INVALIDATION FRAMEWORKOur framework has an offline and an online component (Fig. 1). The offline component is responsible for reflecting document updates on the index and deciding on stale termsCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.We set the timestamp of newly added documents to the current date. For all deleted documents, we set T S to an infinite value. Finally, for a revised document, we compare the old and new versions of the document and set the timestamp to the new version's date only if their lengths (the total number of terms) differ by more than a fixed percentage L (this is similar to [1]). Since each document is assigned to a certain index node via a hash function, we store a document's T S value only on the associated index node.3WWW 2011 ­ PosterFor each term in the index (on each node), we keep an update counter that can be incremented whenever the term's posting list is modified by addition or deletion of postings (an update is modeled as a deletion followed by an addition). Here, we only consider the modifications due to postings that are newly added to a term's list. When the value of a term's update counter exceeds a certain fraction (F ) of its initial posting list length, the term is considered to be stale. Then, a new timestamp is assigned to the term, and its update counter is set to zero.0.24 0.20March 28­April 1, 2011, Hyderabad, IndiaStale traffic ratio0.16 0.12 0.08 0.04 0.00 0.0TTL=1,2,3,4,5 (baseline) L=2.5%, F=10%, TTL=2,3,4,5 L=1%, F=10%, TTL=2,3,4,5 L=0%, F=10%, TTL=2,3,4,52.2 Query Result Invalidation PolicyIn case of a cache hit, the triplet q, R, T S(q) is sent to all nodes (Fig. 1). Then, each node concurrently decides whether the cached result is stale or not. A result is considered to be stale if one of the two conditions hold: · C1: If d R, s.t. T S(d) > T S(q) (i.e., document is deleted or revised after result generation), or · C2: If t q, T S(t) > T S(q) (i.e., all query terms appear in new documents after result generation) If at least one node decides that the result is stale, the query is re-executed and T S(q) is updated. Otherwise, the cached result is served to the user. Note that the first condition of our policy can correctly identify all results that are stale due to deletion of documents in R. For result documents whose scores may have changed due to an update, we take a pessimistic approach. If a document in R is found to have a larger T S value than the query's T S value, we assume that its rank in R is likely to change and predict the query result as stale. The second condition is intended to (partially) handle the stale results that are caused by a newly added document or an updated document (e.g., after addition of query terms) that was not in R, but now qualifies to enter R. For this case, we take a conservative approach and consider only newly added posting for each term (since deletions are mostly handled by the first condition) and predict a query as stale if each one of its terms now appear in a sufficiently large number of new documents. We note that our approach is approximate and may miss some invalidations. We anticipate that such cases would be rather rare in practice. Nevertheless, to handle such cases and prevent accumulation of stale results in the cache, we adapt the practice in [1] and couple our policy with an invalidation scheme based on TTL. Thus, in case of cache hit, the T S of a query is first compared to a fixed TTL value, and if it is found to be expired, it is re-executed. Otherwise, our timestamp-based invalidation policy is applied.0.20.40.60.81.0False positive ratioFigure 2: Prediction accuracy (the TTL value increases from right to left in each curve). update policy, we experiment with L values of 0% (i.e., all revisions to a document cause an update on TS values), 1%, and 2.5%. For the term TS update policy, we set F to 10%. According to Fig. 2, our invalidation policy is considerably better than the baseline TTL approach. In particular, for each TTL point, we have a better ST ratio with a similar or lower FP ratio. For instance, when TTL is set to 2, an ST ratio of 7% is obtained with an FP ratio of 37%. Our policy almost halves this ST ratio (i.e., around 4%) for a lower FP ratio of 36%. As expected, for larger values of L, we obtain fewer redundant executions but higher ST ratios.4.CONCLUDING DISCUSSION3. EXPERIMENTSOur setup is very similar to [1]. We obtain the Wikipedia snapshot on Jan 1, 2006 as well as all added, revised, and deleted documents in the next two months. We also obtain, from the AOL query log, 10K queries that have at least one clicked answer in the Wikipedia domain. Queries span a period of two weeks, and there are 8630 unique queries. Each day, all document updates are processed in batch, and then, the same set of 10K queries are executed over the updated index to retrieve the top-10 results (i.e., the ground truth). We also obtain staleness predictions from our strategy and the TTL strategy for each day (after the updates) and query. We evaluate invalidation strategies in terms of the stale traffic (ST) ratio and false positive (FP) ratio, i.e., percent of redundant query executions [1]. For the document TSOur work achieves prediction accuracies comparable to [1] (e.g., see [1, Fig. 6]) while the relative improvement of [1] over the TTL scheme is better than ours. However, our goal here is to devise an efficient and practical invalidation policy while providing a prediction accuracy better than the basic TTL approach and comparable to [1]. In this respect, our work has significant efficiency advantages over [1]. First, our invalidation framework operates in a distributed manner, i.e., each index node updates document and term timestamps for its own subset of the collection (offline) and checks staleness of results in case of a cache hit (online). In contrast, [1] involves one or more centralized invalidation predictors that find all matching queries in the cache to every revised (added or updated) document (offline), which may cause a bottleneck in the system. Second, the network cost of our policy involves the transfer of q, R, T S(q) triplets between the cache and index nodes for cache hits, whereas the approach in [1] must transfer all synopses for revisions from the parser and further interact with the result cache to find the matching queries. Finally, the timestamp-based invalidation policy has the processing cost of comparing |R|+|q| timestamps, whereas the approach in [1] requires expensive score computations between all revised document synopses and matching queries in the cache.5.4
2	marketing online_display_advertising time_series	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaMeasuring the Effectiveness of Display Advertising: A Time Series ApproachJoel BarajasRam AkellaUC, Santa Cruz Santa Cruz CA, USAjbarajas@soe.ucsc.edu akella@soe.ucsc.edu Marius Holtan Brad Null Jaimie Kwon marius.holtan@teamaol.com jaimie.kwon@teamaol.com brad.null@teamaol.com ABSTRACTWe develop an approach for measuring the effectiveness of online display advertising at the campaign level. We present a Kalman filtering approach to deseasonalize and estimate the percentage changes of online sales on a daily basis. For this study, we analyze 3828 campaigns for 961 products on the Advertising.com network. in sales with respect to a base level on a daily basis. We perform this estimation using a Kalman filtering approach. We determine if a change is positive or negative in the trend using hypothesis testing. Finally, we use four success criteria with respect to these results to evaluate the performance of 3828 campaigns on the Advertising.com network.UC, Santa Cruz Santa Cruz CA, USAAOL Research Mountain View CA, USAAOL Research Mountain View CA, USAAOL Research Mountain View CA, USA2. RELATED WORKPrevious work has studied CPA performance on a monthly level [3]. A more scientific approach incorporating a few user features and the number of impressions in a controlled experiment is detailed in [4] where resolution is at weekly level. Our main goal is to measure the effectiveness of campaigns on a daily level. This is important for short campaigns and to provide dynamic online performance estimates.Categories and Subject DescriptorsH.1.0 [Information Systems]: Models and Principles; G.3 [Mathematics of Computing]: Probability and StatisticsGeneral TermsAlgorithms, Economics, Management, MeasurementKeywordsMarketing, Online Display Advertising, Time Series3. METHODOLOGYWe follow a time-series approach to decompose the daily number of sales of a given product into a weekly seasonal component and a trend component using Kalman filtering. Fig. 1 shows an example of the number of sales for a given product over a thirteen month window. As shown, there is a strong weekly seasonal component which should be removed. Let yt be the number of sales for a given product at time t for t = 1, . . . , T . Assuming yt is normally distributed, we assume the evolution of a latent state t to be a stochastic process describing the true behavior of the series. Then, we can write the Kalman filter as follows: y t = F t + t t = Gt-1 + wt t N (0, V ) wt N (0, W )1.INTRODUCTIONOnline display advertising is an area of rapid growth and consequently of great interest as a marketing channel. Recent studies show that display advertising often triggers online users to search for more information about commercial products [1]. Eventually, many of these users perform either online conversions at the advertiser's website or offline conversions at a physical store. One key challenge is measuring the effectiveness of display advertising in such cases, in particular when users are exposed to multiple advertising channels. If a user performs a commercial action, how should the advertiser attribute credit for the conversion across these multiple channels and media impressions? This is crucial when the business model is cost per action (CPA). In this paper, we address the estimation of the effects of display advertising. We first remove the seasonal (weekly) component of sales. We then estimate the percentage change Main contact. The author is partially supported by CONACYT under the UC-MEXUS agreement grant 194880.Copyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.t is the observational noise with variance V and wt represents the state evolution with covariance matrix W . Given V , W , and the prior mean and variance for 1 , we perform the state estimation by using Kalman filtering equations.1 By fixing G and F , we model the series evolution as a linear combination of a seasonal component and a polynomial component (seasonal trend) [5]. Our goal is to deseasonalize the series to associate the trend with the marketing campaigns. We estimate the Maximum Likelihood Estimate (MLE) of the variances (V, W ) through an Expectation Maximization (EM) approach [2]. Assuming known variances, we estimate1For the full Kalman filtering expressions, see [5] page 103.7WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaFigure 1: Product sales trend example.X-axis represents time in dates. Y -axis represents number of actions (normalized). the distribution of the latent states P (t |y1:t ) for t = 1, . . . T with Kalman filtering equations. We then optimize the likelihood after replacing the expected values for each state.2 Finally, we smooth the time series by estimating P (t |y1:T ) for t = 1, . . . T [5]. Fig. 1 shows an example of the smoothed stochastic process for the trend component. We measure the effectiveness of campaigns by comparing sales occurring during the days of the campaign flight to a baseline. We also test statistical significance of the change in the trend component against the baseline. We use 95% confidence intervals to detect an increase, decrease, or no change in the trend. In addition, we estimate the average change during the campaign duration.Figure 2: Distribution of average change in sales percentage for all campaigns. X-axis represents sales percentage change. Y -axis represents percentage of campaigns.Criterion Criterion Criterion Criterion 1 2 3 4 55% 65% 73% 84% 60% 42% 40% 31%Successful Campaigns (%) Average Increase of Sales: Successful Campaigns4.RESULTSWe analyze 3828 campaigns for 961 products during the period from July 1st , 2009 to July 31st , 2010. We use the previous day of the start of a campaign as the baseline. To evaluate the performance of these campaigns, we define four success criteria: 1. Average positive increase in sales during the campaign 2. Increase or no statistically significant change in sales on every day of the campaign 3. More days with increasing sales than with decreasing sales 4. More days with increasing sales or no significant change than with decreasing sales Results using these criteria are summarized in Table 1. Notice that 65% of campaigns have no statistically significant decrease in the number of sales for any day during the campaign, thus at least maintaining the same latent sales level. This is the primary objective of many campaigns. Fig. 2 depicts the distribution of the average change in sales by campaign. This is broken down into campaigns with positive, negative and no statistically significant change.Table 1: Percentage of successful campaigns and average increase for each criterion. sales against the sales generated during some period prior to the campaign might not always be appropriate. For instance, the campaign may focus on driving offline sales. Or, if the campaign is launched in the off-season, say after Christmas, then comparing sales against sales generated during the Christmas season is not likely to be the right objective. Therefore, negative sales changes as measured in this study do not necessarily mean that the campaign failed. To select the appropriate baseline for these and similar cases, it is necessary to obtain a detailed understanding of the campaign. This is left for future work. Finally, in this study we did not incorporate the number of impressions served in total or on a per user basis, nor did we consider advertising targeting in terms of web user attributes and advertising context including campaign and ad features. We propose to incorporate these components in future work.5.DISCUSSION AND FUTURE WORKOur main goal is to measure the impact of advertising on sales, both near and long term. There are several difficulties we encounter which complicate this goal. First, advertisers typically use all available channels (TV, radio, print, online) and there are many vendors within each channel. Data is typically not integrated across channels or even within channels and thus the idea of following the online click-stream behavior of web users is generally not possible over the whole online channel. In addition, as this is a high level study across several thousands of campaigns, we do not have a detailed understanding of the exact goals of each campaign. Comparing2For details of the optimization see [2].8
5	relation_extraction domain_adaptation web_mining	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaFrom Actors, Politicians, to CEOs: Domain Adaptation of Relational Extractors using a Latent Relational MappingDanushka BollegalaThe University of Tokyo Hongo 7-3-1, Tokyo 113-8656, JapanYutaka MatsuoThe University of Tokyo Hongo 7-3-1, Tokyo 113-8656, JapanMitsuru IshizukaThe University of Tokyo Hongo 7-3-1, Tokyo 113-8656, Japandanushka@iba.t.utokyo.ac.jp ABSTRACTmatsuo@biz-model.t.utokyo.ac.jpishizuka@i.u-tokyo.ac.jpWe propose a method to adapt an existing relation extraction system to extract new relation types with minimum supervision. Our proposed method comprises two stages: learning a lower-dimensional projection between different relations, and learning a relational classifier for the target relation type with instance sampling. We evaluate the proposed method using a dataset that contains 2000 instances for 20 different relation types. Our experimental results show that the proposed method achieves a statistically significant macro-average F -score of 62.77. Moreover, the proposed method outperforms numerous baselines and a previously proposed weaklysupervised relation extraction method.Categories and Subject DescriptorsH.3.3 [Information Systems]: Information Search and Retrievalwe could somehow adapt an existing relation extraction system to those new relation types using a small set of training instances. We study relation adaptation ­ how to adapt an existing relation extraction system that is trained to extract some specific relation types, to extract new relation types in a weakly-supervised setting. We define Relation Adaptation as the problem of learning a classifier for a target relation type T , for which we have a few entity pairs as training instances, given numerous entity pairs for some N source relation types, S1 , . . . , SN . We use the notation = {S1 , . . . , SN , T } to denote the set of all relations. A particular relation type from this set is denoted by R (i.e R ). An entity pair that consists of two entities A and B is denoted as (A, B). Moreover, we use the notation (A, B) R to indicate that the relation R exists between two entities A and B.2.METHODGeneral TermsAlgorithmsKeywordsRelation Extraction, Domain Adaptation, Web Mining1.INTRODUCTIONThe World Wide Web contains information related to numerous real-world entities (e.g. persons, locations, organizations, etc.) interconnected by various semantic relations. Accurately detecting the semantic relations that exist between two entities is of paramount importance for information retrieval (IR). For example, to improve coverage in information retrieval, a query about a particular person can return documents describing the various semantic relations that the person under consideration has with other related entities. Recent work on relation extraction has demonstrated that supervised machine learning algorithms coupled with intelligent feature engineering provide state-of-the-art solutions to this problem [3]. However, supervised learning algorithms depend heavily on the availability of adequate labeled data for the target relation types that must be extracted. Considering the potentially numerous semantic relations that exist among entities on the Web, it is costly to create labeled data manually for each new relation type that we want to extract. Instead of annotating a large set of training data manually for each new relation type, it would be cost effective ifCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.Given a pair of entities (A, B), the first step is to express the relation between A and B using some feature representation. Lexical or syntactic patterns have been successfully used in numerous natural language processing tasks involving relation extraction such as extracting hypernyms [4]. Following the previous work on relation extraction between entities, we use lexical and syntactic patterns extracted from the contexts in which two entities co-occur to represent the semantic relation that exists between those entities. First, we download Web snippets for the AND query of the two entities A and B. Next, we replace A and B respectively by two variables X and Y . Finally, we generate subsequence patterns that contain both X and Y . We extract both lexical and syntactic subsequence patterns using an improved version of the subsequence pattern mining algorithm proposed by Bollegala et al. [2]. We propose a strategy for selecting relation independent patterns using the entropy of a pattern over the distribution of entity pairs. The proposed strategy is inspired by the fact that if a pattern is relation-independent, then its distribution over the entity pairs tends to become more uniform. However, if a pattern is relation-specific, then its distribution is concentrated over a small set of entity pairs that belong to a specific relation type. The entropy of a pattern increases as its distribution becomes more uniform. We construct a bipartite graph, G = (VRS VRI , E) between relation-specific (VRS ) and relation-independent (VRI ) patterns to represent the intrinsic relationship between those patterns. Each vertex in VRS corresponds to a relation-specific pattern, and each vertex in VRI corresponds to a relation-independent pattern. A vertex in VRS (corresponding to a relation-specific pattern) is connected to a vertex in VRI (corresponding to a relation-independent pattern) by an undirected edge eij E. Note that there are no intra-set edges connecting vertices in VRS and VRI . Moreover,13WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaFrom Table 1, we see that the proposed method has the best macro-average F -measure among all the different methods. In particular, improvement against the previously proposed state-of-theart weakly-supervised relation extraction method [5] is statistically significant (paired t-test with p < 0.05 inferred as significant). The Random baseline on this balanced dataset only yields a very low F -score of 7.25. The RI patterns baseline that uses only relationindependent patterns outperforms the RI patterns baseline that uses only relation-specific patterns. Using all the patterns (i.e. All patterns baseline) performs slightly worse than when using only relation-independent patterns. One reason for this is that the overall performance of the All patterns baseline is dominated by the numerous relation-specific patterns, which adapt poorly to target relations. There can be errors in identifying relation-independent patterns using strategies such as mutual information, which engender some noise in the constructed bipartite graph. Consequently, using only the Projected features is not satisfactory. However, by augmenting the original features to the projected features (i.e. Combined baseline), this problem can be overcome. Next, we evaluate the effect of the one-sided undersampling on top of the numerous baselines discussed above. From Table 1, it is apparent that, by sampling, we consistently improve all the baselines: RS patterns, RI patterns, All patterns, and Projected. In fact, the proposed method, which uses augmented feature vectors with sampling, shows a 6 percent improvement over not using sampling (i.e. Combined baseline). Moreover, we experimentally verified that the proposed method performs consistently under different parameter settings.Table 1: Macro-average results for various methods.Method Random RS patterns RI patterns All patterns Projected Combined (all patterns + projected) RS patterns + Sampling RI patterns + Sampling All patterns + Sampling Projected + Sampling Jiang [5] Combined + Sampling (PROPOSED) F-measure 7.24 41.41 51.40 47.94 44.86 56.99 49.78 54.83 57.62 47.61 55.62 62.77each edge eij E is associated with a non-negative weight mij , that measures the strength of association between the corresponding patterns i and j . We set mij to the number of different entity pairs from which both i and j are extracted. Edge weights mij are represented collectively by an edge-weight matrix M of the bipartite graph G. For simplicity, we use the number of different entity pairs from which two patterns are extracted as the edgeweighting measure.Given as input an edge-weight matrix M for the bipartite graph G and dimensionality k(< n) of the latent space, we use spectral clustering to compute a projection matrix U from the original n dimensional pattern space to a k dimensional latent space. The low-dimensional projection reduces the mismatch between patterns in source and target relation types, thereby enabling us to train a classifier for the target relation type using labeled entity pairs for both source and target relation types. In relation adaptation, the number of target relation training instances (entity pairs) is significantly smaller than that of the source relations. Consequently, most supervised classification algorithms treat the minority class (target relation) instances as noise or outliers. Therefore, learning a classifier for a target relation type which has only a few instances is difficult in practice. To overcome this problem, we use one-sided under-sampling which first selects a subset of the source relation training data and then uses that subset to train a multi-class classifier. One-sided under-sampling methods have been used to select a subset of the majority class in previous work investigating the problem of machine learning with unbalanced datasets [6, 7].4.CONCLUSIONWe proposed and investigated a method to learn a relational classifier for a target relation using multiple source relations. Our experimental results show that the proposed method significantly outperforms 10 baselines and a previously proposed weakly-supervised relation extraction method on a dataset that contains 2000 entity pairs for 20 different relation types. Both feature projection and sampling positively contribute to the proposed method. In future studies, we intend to apply the proposed method to other classification tasks.5.EXPERIMENTS AND RESULTSTo evaluate the proposed method, we select 20 relation types that have been used frequently for evaluating relation extraction systems [1, 2] from the YAGO ontology1 . For each selected relation, we randomly selected 100 entity pairs listed for that relation in the YAGO ontology. Overall, the dataset contains 2000 (20 relations × 100 instances) entity pairs. The YAGO ontology has a high level of manually confirmed accuracy. It is suitable as a gold standard for evaluating relations between entity pairs on the Web [8]. For each relation type R, we randomly allocated its 100 instances (entity pairs) into three groups: 60 instances as training instances when R is a source relation, 10 instances as training instances when R is a target relation, and 30 instances as test instances for R. For each target relation type, therefore we have 1140 (19 × 60) source relation training instances and 10 target relation training instances, which well simulates the problem setting in relation adaptation. We repeat the above-described data splitting and report the average results of 5 random times.1[1] M. Banko, M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open information extraction from the web. In IJCAI'07, pages 2670­2676, 2007. [2] D. Bollegala, Y. Matsuo, and M. Ishizuka. Relational duality: Unsupervised extraction of semantic relations between entities on the web. In WWW'10, pages 151 ­ 160, 2010. [3] Z. GuoDong, S. Jian, Z. Jie, and Z. Min. Exploring various knowledge in relation extraction. In ACL'05, pages 427 ­ 434, 2005. [4] M. Hearst. Automatic acquisition of hyponyms from large text corpora. In COLING'92, pages 539­545, 1992. [5] J. Jiang. Multi-task transfer learning for weakly-supervised relation extraction. In ACL'09, pages 1012­1020, 2009. [6] M. Kubat and S. Matwin. Addressing the curse of imbalanced training sets: one-sided selection. In ICML'97, pages 179 ­ 186, 1997. [7] F. Provost. Machine learning from imbalanced data sets. In AAAI'00 Workshop on Imbalanced Data Sets, 2000. [8] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A core of semantic knowledge. In WWW'07, 2007.http://www.mpi-inf.mpg.de/yago-naga/yago/14
6	query_recommender_systems web_search_effectiveness	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaRecommendations for the Long Tail by Term-Query GraphFrancesco BonchiYahoo! Research, BCN, SpainRaffaele PeregoISTI - CNR, Pisa, ItalyFabrizio SilvestriISTI - CNR, Pisa, Italybonchi@yahoo-inc.com r.perego@isti.cnr.it f.silvestri@isti.cnr.it Hossein Vahabi Rossano VenturiniIMT, Lucca, Italy ISTI - CNR, Pisa, Italyhossein.vahabi@imtlucca.it ABSTRACTWe define a new approach to the query recommendation problem. In particular, our main goal is to design a model enabling the generation of query suggestions also for rare and previously unseen queries. In other words we are targeting queries in the long tail. The model is based on a graph having two sets of nodes: Term nodes, and Query nodes. The graph induces a Markov chain on which a generic random walker starts from a subset of Term nodes, moves along Query nodes, and restarts (with a given probability) only from the same initial subset of Term nodes. Computing the stationary distribution of such a Markov chain is equivalent to extracting the so-called Center-piece Subgraph from the graph associated with the Markov chain itself. Given a query, we extract its terms and we set the restart subset to this term set. Therefore, we do not require a query to have been previously observed for the recommending model to be able to generate suggestions.r.venturini@isti.cnr.ita user has issued q2 after q1 . Furthermore, e is weighed by the probability of a user to issue q2 after q1 . Given a query q, suggestions are generated by means of random walks from q on the QFG [3]. All these previous models suffer from the same limitation: they can only provide recommendations for queries present in the training query log. For instance, the QFG model needs query q to be present in the graph in order to start the random walk and provide recommendations. As it was pointed out by Downey et al. [4] trough an analysis on search behaviors, rare queries are very important, and their effective satisfaction is very challenging for search engines. Therefore, it is even very important to provide good recommendations for long-tail queries. A recent work by Yang et al. [6] proposes an optimal framework for rare-query suggestion leveraging on implicit feedbacks from users mined from the query logs. However, unknown queries are excluded even in this context. In this paper instead, we introduce a model enabling the generation of query suggestions also for previously unseen queries: to the best of our knowledge this is the first model able to do so. The model is briefly explained next.Categories and Subject DescriptorsH.3.3 [Information Search and Retrieval]: Search process, Query formulationGeneral TermsAlgorithms, Experimentation, Measurement2. TERM-QUERY GRAPH MODELLet Q be a query-log, i.e. a sequence of queries q1 , . . . , qn . Moreover, let Q be the set of queries in the query-log. Each query is made up of terms from a vocabulary T , without loss of generality we consider each query q as a subset of the term set T , i.e. q T . From T and Q we build a digraph in the following manner. Each term t T and query in q Q is a node, we denote by VT (resp. VQ ) the set of Term (resp. Query) nodes, i.e. nodes corresponding to terms in T (resp. Q). Likewise, we define two kinds of edges. We denote by ET Q the set of directed edges going from terms in T to queries in Q, t q ET Q iff t q. We denote by EQ the set of direct edges among queries, i.e. q1 q2 EQ iff the likelihood a user submits q2 after q1 is not null. Resuming, we consider a digraph G = (VT VQ , ET Q EQ ), where the subgraph GQ = (VQ , EQ ) is, basically, the QueryFlow Graph (QFG) [2] built on queries from Q. We call this graph the Term-Query Graph, or TQ-Graph for short, and we consider this as the reference structure in our TermQuery Graph Model. Query suggestions for a query q = {t1 , . . . tm } T are generated, within this model, by extracting the Center-piece Subgraph starting from the m Term nodes t1 , . . . , tm [7]. Therefore, in practice, we compute m random walks withKeywordsQuery Recommender Systems, Web Search Effectiveness1.BACKGROUND AND RELATED WORKQuery recommendations are an important tool for search engines, as they help users in refining their initial queries towards a better expression of their information needs. Recommendations are typically queries semantically related to the original one, and they are usually obtained by models learned with query logs [5], for instance, by clustering queries [8], or by identifying frequent re-phrasings [1]. Recently, Boldi et al. introduced the Query Flow Graph [2] (QFG) model, which is a Markov chain-based representation of a query log. A QFG is a directed graph in which nodes are queries, and an edge e = (q1 , q2 ) exists if at leastCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.15WWW 2011 ­ Posterrestart from each term in q. Let r(1) , . . . , r(m) be the corresponding stationary distributions. Furthermore, let r = r(1) r(2) . . . r(m) be the Hadamard product of the m vectors, i.e. ri =mMarch 28­April 1, 2011, Hyderabad, Indiaby querying for "Bill Clinton". Even if this query is not in the long tail, and we have a query node Bill Clinton in VQ , it is interesting to observe that, by splitting the query in terms, computing the random walks, and then combining the results, we do not loss precision. In fact, we notice that all the top 5 queries are related to the input query. Moreover, we observe that the results are highly diversified: we have clinton's speeches, scandal, foundation and biography. Query: Bill Clinton Suggested Query president bill clinton speeches famous bill clinton quotes monica lewinsky and bill clinton scandal bill clinton foundation website former president bill clinton biography Score 8.8 e-11 8.4 e-11 8.4 e-11 8.3 e-11 8.1 e-11corresponds to a query in Q, we recommend the queries corresponding to the top scoring ones. As an example, consider a query-log containing just two queries, q1 , q2 , whose terms are in a vocabulary of three terms, t1 , t2 , t3 . Suppose after we ran the above random walk with restart procedure restarting from each one of the three terms we have the following three stationary distributions: 1 = [0.9, 0.1]; 2 = [0.3, 0.5]; 3 = [0.09, 0.91]. If a user submit a query (t1 , t3 ) the system will suggest the query q2 since the vector of scores r as defined above results to be [0.081, 0.091], thus q2 ranks higher in this case.k=1r(k) i . Since each dimension of r3.LONG-TAIL RECOMMENDATIONS4. EFFICIENCYIt is needless to say that the suggestions must be computed efficiently since a recommender system should work online. Our effort and contribution in this project is also devoted to the efficiency aspect, as it will be described in an extended version of this work. We can anticipate here that our model is particularly suitable for efficient implementation. In fact, we can store precomputed information enabling to efficiently compute suggestions in a way that recall the process of intersecting posting lists. More in details, we store the stationary distribution of each random walk for each term as it was a posting list by sorting queries' ids by their scores. We also adopt state-of-the-art bucketing, approximations and thresholding techniques in order to considerably reduce the space requirements. Finally the recommendations for a given query are computed by scanning and intersecting the posting lists corresponding to its terms.We next report few examples from a preliminary assessment of the model. We will report a deeper analysis in an extended version of the paper. We build our model starting from a 6 months anonymized query logs (from January to June 2010) from Yahoo! US. The resulting TQ-Graph consists of 6,261,105 Term nodes and 28,763,637 Query nodes. We have 83, 808, 761 edges from Term nodes to Query nodes and 56, 250, 874 edges from Query nodes to Query nodes. We next report recommendations for queries that are not present in our query log, thus queries for which previously known methods could not provide recommendation. We report the first five results obtained for each query. Query: Social Katz index Suggested Query katz index of activities of daily living modified barthel index barthel index score modified barthel index score youtube Score 2.7 e-09 8.6 e-13 6.4 e-13 8.8 e-14 4.3 e-19AcknowledgementThis research has been partially funded by the EU CIP PSPBPN ASSETS Project. Grant Agreement no. 250527.The first query shown is Social Katz index. The first four suggested queries are highly related with the input query. Both Katz index and Barthel index are indeed indexes used to measure performance in basic activities of daily living. The last query suggested is not related with the query. However, we observe that its score is at least roughly five orders of magnitude smaller than the one of other results. The second query shown is Menu restaurant design. In this case all the reported queries are related to the input query. It is interesting to notice that the third result is restaurant menu design that is a more correct way to formulate the same query. Our method is insensitive to permutation of terms in the query. Query: Menu restaurant design Suggested Query free restaurant design software free restaurant kitchen layout and design restaurant menu design restaurant menu design software restaurant menu design samples Score 4.9 e-12 4.8 e-12 4.7 e-12 4.7 e-12 4.4 e-12Obviously the method must perform reasonably well also for head-queries. Next we show the first 5 results obtained16
9	value reasoning	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaSurvivability-Oriented Self-Tuning of Web Systems1Bihuan Chen1, Xin Peng1, Yijun Yu2, Wenyun Zhao1School of Computer Science, Fudan University, Shanghai 201203, China 2 Department of Computing, The Open University, Milton Keynes, UK{09210240005, pengxin, wyzhao}@fudan.edu.cn, y.yu@open.ac.uk ABSTRACTRunning in a highly uncertain and changing environment, Web systems cannot always provide full set of services with optimal quality, especially when the workload is high or failures in subsystems occur frequently. It is thus desirable to continuously maintain a high satisfaction level of the system value proposition, hereafter survivability assurance, while relaxing/sacrificing certain quality/functional requirements that are not crucial to the survival of the Web systems. In this paper, we propose a requirements-driven self-tuning method for survivability assurance of Web systems. Using a value-based feedback controller plus a requirements-oriented reasoner, our method makes both quality and functional requirements tradeoffs decisions at runtime. deoffs concern when and which desired functional services can be unbound to ensure the critical and essential services and their quality, and such unbound services can be bound to preserve the functional integrity whenever it is possible. To make such functional tradeoffs decisions, we use a reasoning algorithm that is performed on an enriched requirements goal model. Then we interpret survivability from the perspective of value-based software engineering [5] as the capability of maximizing the satisfaction level of the system value proposition, which defines how the "earned business value" at system side is measured to facilitate the "when" part of the problem. On the other hand, the goal models enriched with annotations of value contributions facilitate the "which" part of the problem.Categories and Subject DescriptorsD.2.10 [Software Engineering]: Design ­ methodologies.2. OUR METHOD 2.1 Value-Based SurvivabilityWe use goal modeling [6] to provide a formal representation of runtime requirements to facilitate reasoning. The goal model for an online shopping system is presented in Figure 1. Observing that some hard goals in rounded rectangles can be unbound without influencing the achievement of their parent goals, e.g., customers can partially achieve Online Shopping even if Make Review is unbound from the system. We enrich AND/OR decompositions of the goal model with the relative parent value to reflect the relative importance of sub-goals or their value contribution to parent goals from a business perspective. For simplicity, the relative parent value is defined as a ratio between 0.0 and 1.0. When it is 1.0, the goal must be retained to achieve the value of its parent goal; otherwise, the value of the parent goal can still be partially achieved even when this goal is unbound. The value contribution annotations are provided by business experts to deal with economic and marketing factors such as consuming behaviors of customers.General TermsAlgorithms, Reliability, Measurement, PerformanceKeywordsSurvivability, Self-Tuning, Value, Requirements, Reasoning1. INTRODUCTIONFailing to have such a Web system that is reliably working and resilient to changes in the running environment, the business can lose their customers because it is easy for customers to transfer to competing Web sites [1]. On the other hand, the uncertainty and unpredictability of the running environment of Web systems makes it hard to constantly provide full set of services with optimal quality, especially when the workload is high or failures in subsystems occur frequently. Rather than absolute reliability, it is therefore survivability that is more practical and reasonable for Web systems. Here survivability is the capability of ensuring critical system services under adverse conditions, with acceptable quality degradation or even sacrifice of some desired services [2]. In this work, we show that survivability can be assured by autonomic runtime reconfigurations rather than by error-prone human intervention on the runtime structures and behaviors of the software system. Furthermore, survivability can be interpreted quantitatively to guide the reconfigurations precisely. In order to achieve autonomic survivability assurance, we adopt the MAPE (Monitor, Analyze, Plan, Execute) control loop [3] and propose a requirements-driven self-tuning method. Extending our previous work on runtime tradeoffs about quality requirements [4], the proposed method further takes into account runtime tradeoffs about functional requirements. Quality tradeoffs concern when and which desired quality requirements may be relaxed to ensure other more critical but conflicting ones. Similarly, functional traCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.Figure 1. A goal model enriched with relative parent values.23WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaTo gauge the satisfaction level of system value proposition, it is required to measure the earned business value formally. However, many factors can influence the earned business value in a direct or indirect way. In our example, sales is a direct factor while products' details, recommended products, customers' reviews and consultations are indirect factors. Hence, a formula should be provided by a comprehensive analysis on such complicated factors by marketing experts. To illustrate the point, we simplify the formula in our example by considering the above 5 factors: each corresponds to a kind of runtime transaction that produces certain profit, e.g., a transaction of making review produces $0.032.rations that reach the satisfaction levels of high-ranked soft goals while relaxing the satisfaction levels of low-ranked soft goals. Here every configuration is a selection of the OR-decomposition goals. (p2) If no new configurations are found by the quality goal reasoner, the functional goal reasoner is triggered to generate a reconfiguration that optimizes the satisfaction levels of high-ranked soft goals while sacrificing one leaf-level hard goal with the minimal relative root value defined as the product of the relative parent values along the path from root goal to the hard goal. (p3) When the earned business value increases by a certain degree, the functional goal reasoner is triggered to generate a reconfiguration that improves the functional integrity of the Web system by rebinding the recently sacrificed leaf-level hard goal. Note quality tradeoffs decisions are made to replace one component with another and the functional tradeoffs decisions are made to unbind or bind a component. Comparing with a replacement, unbinding or binding introduces more radical change to the architecture. And there is often an uncertainty in whether or not such a radical action has taken effect on the target system [7]. As a result, the system could suffer oscillations from such unbinding and binding actions (e.g., alternating the same component). Thus we adopt a timed delay for the effect of the action to be achieved in functional tuning execution as suggested by S. W. Cheng et al. [7].2.2 Self-Tuning FrameworkTo achieve the requirements-driven self-tuning of Web systems, our method postpones the design-time quality and functional tradeoffs decisions to the runtime, and adapts them autonomically in response to changes in the environment in order to maximize the satisfaction level of system value proposition. Figure 2 presents our self-tuning framework, illustrating the main components and their mappings to the MAPE control loop.3. CONCLUSIONSWith our value-based interpretation of survivability assurance, we have proposed a requirements-driven self-tuning method for survivability assurance of Web systems. The method employs both quality and functional requirements tradeoff through goal-oriented reasoning. We intend to investigate the timed delay and its impact more deeply and more precisely in the near future and to apply our method in the new paradigm of cloud computing.Figure 2. Our self-tuning framework. Monitor. We use sensors to collect runtime data, e.g., the failure/success of a payment service, etc., through logging records; Analyze. By analysis of the runtime data, we use value indicators to measure the earned business value as shown in Section 2.1, and obtain the quality measurements, e.g., availability of the payment service, etc., of related quality requirements; Plan. Based on analyze, we use the PID controller to decide whether or not to make quality or functional tradeoffs decisions to maximize the satisfaction level of system value proposition. In the former case, the PID controller is also used to rank the preference of related soft goals to guide the reasoning for a reconfiguration to the goal model; Execute. Finally, we use the goal model configurator to reconfigure the goal model according to the planned goal reconfiguration, and then we use the architecture configurator to execute the adaptation by reconfiguring the runtime architecture according to the mappings between the goals and the architectural components. Such architecture reconfigurations are currently supported by a reflective component model. The monitor runs all the time while analyze, plan and execute run iteratively at regular time unit, e.g., per every minute. Furthermore, there are three possible planning paths. (p1) When the earned business value decreases by a certain degree, the quality goal reasoner is triggered to generate a set of configu-4. ACKNOWLEDGMENTSThis work is supported by National Natural Science Foundation of China under Grant No. 90818009.24
10	crowdsourcing social_media	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaLearning Facial Attributes by Crowdsourcing in Social MediaYan-Ying ChenNational Taiwan UniversityWinston H. HsuNational Taiwan UniversityHong-Yuan Mark LiaoInstitute of Information Science, Academia Sinicayanying@gmail.comwinston@csie.ntu.edu.twliao@iis.sinica.edu.twABSTRACTFacial attributes such as gender, race, age, hair style, etc., carry rich information for locating designated persons and profiling the communities from image/video collections (e.g., surveillance videos or photo albums). For plentiful facial attributes in photos and videos, collecting costly manual annotations for training detectors is time-consuming. We propose an automatic facial attribute detection method by exploiting the great amount of weakly labelled photos in social media. Our work can (1) automatically extract training images from the semantic-consistent user groups and (2) filter out noisy training photos by multiple mid-level features (by voting). Moreover, we introduce a method to harvest less-biased negative data for preventing uneven distribution of certain attributes. The experiments show that our approach can automatically acquire training photos for facial attributes and is on par with that by manual annotations.Figure 1: Leveraging photos from social groups for training facial attributes. (a) Photos from the groups. (b) Rejecting noisy images by group quality and (mid-level) feature voting. (c) Ensemble learning by the automatically collected training images. standard face benchmark. Laboratory data generally fails to cover the rich facial appearances in consumer photos. Kumar et al. [2] trained 10 attributes by manually labelling certain images collected from the internet. Crowdsourcing ­ exploiting enormous user-contributed images and tags from the web ­ has been shown promising for easing the pains for collecting the vital annotations for supervised learning. For example, Fergus [1] preliminarily introduced internet images for image classification. Ni et al. [4] constructed an age estimator using web images and text information. Although crowdsourcing tends to be effortless, the quality for annotations (images) is questionable. Different from the previous works for crowdsourcing, we leverage the social groups from the community-contributed photo services (e.g., Flickr). It is natural since such photos are rich in facial attributes across races and countries. Meanwhile, we locate semantic-consistent photos from the designated user groups retrieved by keywords for the target facial attributes. To ensure annotation quality, we propose an quality measurement to rank social groups of proper semantic correlation and a voting scheme (by multiple mid-level features) to reject noisy training photos. Furthermore, we present an approach to harvest less-biased negative training images to prevent uneven distribution in certain attributes. We show in Section 3 that our approach successfully rejects noisy images for training facial attributes and the accuracy is competitive with learning by manual annotations.Categories and Subject DescriptorsI.4.9 [Computing Methodologies]: Image Processing and Computer Vision - ApplicationsGeneral TermsAlgorithms, ExperimentationKeywordscrowdsourcing, facial attribute, social media1.INTRODUCTIONBesides low-level features for face recognition, the rich set of facial attributes (e.g., gender, race, age, hair style, smile, etc.) has been shown promising for describing target persons or profiling human activities [2]. For example, we can locate a designated person in surveillance videos by (composite) facial attributes (e.g., a bearded man). For market research, we can understand the user's preferences or profiles by analyzing the facial attributes in her photo albums; for example, "most of Alice's friends are Asian girls with long hairs." Prior works for automatic facial attribute detection solely relied on supervised learning with manually collected training photos from limited sources. Moghaddam et al. [3] proposed a gender classification approach using FERET, aCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.2. LEARNING FACIAL ATTRIBUTES FROM SOCIAL GROUPSLearning facial attributes confronts two difficulties: (1) current training data cannot cover the rich diversities of facial attributes ,(2) learning numerous attributes requires huge annotation efforts. To address the problems, we crowd-25WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaTable 1: Detection accuracy (%) for four attributes. Our approach (c) is competitive with that by manual annotations (a) and outperforms that by noisy photos specified by (image-level) keywords only (b).method (a) manual (b) image-level (c) our approach gender 85.63 78.08 83.02 smile 87.04 80.24 86.28 kid 87.88 74.21 85.97 beard 85.17 73.67 81.70 beard(UBD) 82.97Figure 2: Query results in Flickr: (a)Image-level query by "man" (b)Group-level query by "man" (c) Two low-quality social groups (for cats and arts) for facial attribute woman. source photos from community-sharing services since they are abundant, effortless, diverse, and up-to-date. For the quality issue, we crawled the related photos from the social groups with high quality measures and then reject the noisy training photos by mid-level feature voting (cf. Fig. 1). Crawling photos from social groups ­ Social groups accommodate user-contributed photos of the same theme. As shown in Fig. 2(a)(b), group-level query introduces much less noisy photos than image-level query. We retrieve a group list by the keywords pertaining to the facial attributes (e.g., woman). Faces from these photo groups are detected. Measuring group quality ­ Finding relevant groups by keywords suffers from text ambiguity. For example, the group "Cat women" shares cat photos, which are less relevant to the facial attribute, "women." We observe that such irrelevant groups containing fewer faces (cf. Fig. 2(c)). We then take the ratio of face numbers among the whole photos in the group as a quality measure for each group. We select photos from the high quality groups as the candidate (positive) training photos and also balance the number of faces from different groups for ensuring diversity. Removing noisy training photos by feature voting ­ Noisy photos still exist because users tag a photo rather than a face. For instance, the tag "woman" or "man" both incurs noisy photos if they contain a woman and a man. We propose a voting scheme by multiple mid-level features to filter out such noisy photos (cf. Fig. 1(c)). Each mid-level feature is a SVM classifier with varying low-level features (i.e., Gabor filter, Histogram of Gradient, Color, Local Binary Patterns) extracted from each face component (i.e., whole face, eyes, philtrum, mouth). Such mid-level features are rich and carry semantic meanings for facial attributes as shown in [2]. We then reject the candidate training photos with low voting scores. Note that such mid-level classifiers are trained by 5-fold cross-validation from the training photos. Harvesting less-biased negative data ­ For facial attribute classification, randomly picking examples as negative data is impractical because attributes are usually correlated (e.g., "man" and "beard"). We propose two solutions for collecting negative training photos. We use antonyms of the attribute to exploit negative data if the antonyms are explicit. Otherwise, we seek universal background data (UBD), a background training data collected by neutral words such as "people," "persons" that are not specific to any attributes. For each attribute, we also reject photos in UBD that are strongly correlated to (by voting) the mid-level features sincethey might be related to certain attributes. UBD reflects the background photos of facial attributes in consumer photos so that it is less biased than defining specific antonyms for negative training photos. We observed that UBD is effective for the attributes not frequently observed in photos (e.g., beard, sunglasses, etc.). The assumption is reasonable since the attributes appeared frequently (e.g., gender, age) often have explicit antonyms for acquiring negative data. Ensemble learning over mid-level features ­ Provided the crowdsourced training photos, we then adopt Adaboost to aggregate the rich set of mid-level features to construct each facial attribute detector [2].3. EXPERIMENTS AND DISCUSSIONSTackling noisy training photos ­ We compare our approach with (a) learning by manual annotation [2] and (b) learning by noisy images from the web [1]. Each attribute is trained by 1800 positive faces and 1800 negative ones and evaluated on 400 faces. The face images are harvested from Flickr. Table 1 shows the impact of the proposed method. The accuracy by automatically crowdsourcing the social groups and rejecting noisy photos reaches almost similar performances across facial attributes learned by manual annotations. Intuitively, simply crawling by image-level keywords degrades the quality of training photos and thus yields lower accuracy. Effectiveness of UBD ­ For preliminary understanding, we experiment on "beard" attribute since it is not frequently observed in photos and lacks explicit (visual) antonyms. Although antonyms likely discriminate the positive from negative facial attributes, it possibly causes data skew if the antonyms cannot cover the whole attribute space. For example, data is unevenly distributed if we only define "woman" and "kid" as the antonyms for "beard." As Tabel 1(c) shows, UDB (82.97%) outperforms that by negative training photos acquired by antonyms (81.70%) since UBD avoids data skew and does help classification. Leveraging photos in social groups and removing noisy photos by mid-level feature voting, we can automatically acquire effective training photos for facial attribute detection. We also propose UBD to obtain less biased negative training photos. We aim to extend the crowdsourcing methods to a huge set of facial attributes and enable applications (e.g., user profiling and retrieval) by the rich facial attributes.26
11	random_walk ranking	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaGenerating Summaries for Ontology SearchGong Cheng petercheng456@gmail.com Weiyi Ge geweiyi@gmail.com Yuzhong Qu yzqu@nju.edu.cnState Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210093, ChinaABSTRACTThis poster proposes a novel approach for generating summaries for ontology search. Following previous work, we define ontology summarization as the problem of ranking and selecting RDF sentences, for which we examine three aspects. Firstly, to assess the salience of RDF sentences in an ontology, we devise a bipartite graph model for representing the ontology and analyze random walks on this graph. Secondly, to reflect how an ontology is matched with user needs expressed via keyword queries, we incorporate query relevance into the selection of RDF sentences. Finally, to improve the unity of a summary, we optimize its cohesion in terms of the connections between constituent RDF sentences. We have implemented an online prototype system called Falcons Ontology Search.rdfs:subClassOf rdfs:range rdf:types1(b) s2rdf:types3owl:someValuesFrom rdfs:subClassOf owl:onPropertys4Figure 1: Four RDF sentences.Categories and Subject DescriptorsG.3 [Mathematics of Computing]: Probability and Statistics--Markov processes; H.1.2 [Models and Principles]: User/Machine Systems--human factors, human information processing; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing--abstracting methods part of an ontology to enable fast investigation. In this work, we propose a new method for computing salience. Further, we combine it with two other metrics, namely query relevance and cohesion, to form a solution to generating summaries for ontology search. The solution has been applied to an ontology search engine called Falcons Ontology Search (http://ws.nju.edu.cn/falcons/ontologysearch/).General TermsAlgorithms, Human Factors2.PROBLEM STATEMENTKeywordsCohesion, ontology summarization, query relevance, random walk, ranking1. INTRODUCTIONWhen building a Web application, reusing an existing ontology could not only facilitate domain modeling but also place the application on a "semantic bus" where different applications easily interchange the meaning of their semantically homogeneous data. For systems that support ontology reuse such as an ontology search engine, a key feature is how to assist users in finding relevant ontologies efficiently, given that a returned ontology may define a great many term (i.e. class and property) specifications. To this end, ontology summarization [2] has been proposed to extract a salientCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0632-4/11/03.Following [2], the RDF graph representation of an ontology has a unique finest partition that satisfies: two RDF triples that share common blank nodes are in the same part. Each part, being an RDF graph by itself, is called an RDF sentence. For example, Fig. 1 illustrates four RDF sentences. Then a summary of an ontology is defined as a subset of RDF sentences subject to a size constraint in terms of the total number of their constituent RDF triples. In this work, we consider ontology summarization as an optimization problem by introducing an objective function that characterizes the goodness of a summary S of an ontology o w.r.t. a keyword query Q: Goodness(o, S, Q) (1 - - ) · Salience(o, S) + · Relevance(S, Q) + · Cohesion(S) , in which , , + [0, 1] are weighting coefficients. This function linearly combines three aspects, which will be detailed in the next section. (1)27WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, Indiahow the ontology matches the user's needs carried by the query. We achieve this by constructing a textual representation for every RDF sentence and measuring how similar to the query it exhibits. Specifically, let KWSet(s) be the set of normalized (e.g. lowercased) keywords found in RDF sentence s. These keywords come from the local name of every term described by s and the lexical form of every literal that occurs in s. A query Q is also defined as a set of keywords. Thereby, the textual similarity between s and Q could be defined as the "precision" of KWSet(s) in matching Q: TextSim(s, Q) | KWSet(s) Q| . | KWSet(s)| (4)Figure 2: A sentence-term graph.3. METRICSIn this section, we elaborate on the metrics we leverage to rank and select RDF sentences into a query-biased summary of an ontology for being used in the search scenario.Finally, the query relevance of a summary S is given by: Relevance(S, Q) TextSim(s, Q) . (5)sS3.1 SalienceIn an ontology, an RDF sentence describes one or more terms; a term is described by one or more RDF sentences. These lead to a bipartite graph that models such description relationship between RDF sentences and terms, which we call sentence-term graph (STG). Figure 2 illustrates an STG induced by the RDF sentences shown in Fig. 1. Note that we say an RDF sentence describes a term if, in the RDF sentence, the term has an occurrence that is not an instantiation (i.e. as the object of an RDF triple whose predicate is rdf:type, or as the predicate of an RDF triple). For instance, in Fig. 1, s3 describes vin:Region, and thus, in Fig. 2, s3 , vin:Region STG. Different from [2] that operates on an RDF Sentence Graph, here we measure the salience of an RDF sentence by computing its centrality on the STG induced by its ontology. To this end, we adapt the well-known PageRank algorithm for this bipartite graph setting. Specifically, let PRr (s) be the ranking score of an RDF sentence s at state r, which is iteratively updated as follows: PRr+1 (s) 1- +· n {s ,tSTG |s,tSTG}3.3CohesionCohesion indicates the user-perceived unity of a summary. In ontology summarization, it can be achieved by having constituent RDF sentences referring to the same thing, i.e. they are connected by the terms they describe in common. A direct benefit from increasing such interconnections is that, if we visualize a summary as a merge of its constituent RDF sentences, this resulting RDF graph will be less disconnected so that it could better characterize the semantic relationships between terms. Such relationships function as additional information provided to users and may lead to more accurate relevance judgments in search. Specifically, given Describes(s) being the set of terms described by RDF sentence s, we define the connection between two RDF sentences: Connection(si , sj ) | Describes(si ) Describes(sj )| . (6)PRr (s ) , d(s ) · d(t)Finally, the cohesion of a summary S is given by: Cohesion(S) Connection(si , sj ) .si ,sj S si =sj(7)(2) in which (0, 1) is a dumping factor, n is the total number of RDF sentences in o, and d returns the degree of a node in STG. To exploit PageRank, inspired by [1], we actually treat every path of length 2 from one RDF sentence to another in STG as a "link" between them; we assume a surfer, who performs random walks between RDF sentences, at each step either jumps to any RDF sentence or follows a "link" to some RDF sentence; the surfer selects targets always using a uniform probability distribution. Finally, in PageRank it has been proved that PRr (s) converges toward a constant PR (s) that does not depend on any initial values of PR. With PR (s) that represents the centrality of s in the ontology and thus characterizes its salience, we define the salience of a summary S as: Salience(o, S) PR (s) . (3)sS4.CONCLUSION AND FUTURE WORKWe have described a novel approach for summarizing ontologies, and have implemented it in a newly developed ontology search engine. In future work, we will have a more in-depth look at the hypothetical surfer's behavior of random walks on STG. We will also conduct an empirical study to compare this work with existing approaches.5.ACKNOWLEDGMENTSThis work was supported by the NSFC under Grant 60973024. We would like to thank Dr. Xiang Zhang and anonymous reviewers for their invaluable comments.6.In the context of ontology search fed by a keyword query submitted by a user, the selection of RDF sentences should reflect not only the most salient part of an ontology but also[1] L. Lempel and S. Moran. SALSA: The stochastic approach for link-structure analysis. ACM Trans. Inf. Syst., 19(2):131­160, April 2001. [2] X. Zhang, G. Cheng, and Y. Qu. Ontology summarization based on rdf sentence graph. In Proc. WWW, pages 707­716, May 2007.28
12	query_completion query_log	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaQuery Completion without Query Logs for Song SearchNitin DuaIndian Institute of Technology Guwahati, India 781039 +91 9954604398Kanika GuptaMonojit ChoudhuryMicrosoft Research Lab India Bangalore, India 560080 +91 (80) 6658-6000Kalika Balinit.dua@gmail.com{v-kanikg, monojitc, kalikab}@microsoft.comABSTRACTWe describe a new method for query completion for Bollywood song search without using query logs. Since song titles in nonEnglish languages (Hindi in our case) are mostly present as Roman transliterations of the native script, both the queries and documents have a large number of valid variations. We address this problem by using a Roman to Hindi transliteration engine coupled with appropriate ranking and implementation strategies. Out of 100 test cases, our system could generate the correct suggestion for 91 queries.the query log beginning with the string q'. The basic underlying assumption, i.e., popular queries of the past will also be searched commonly in the future, is valid for song search as well. However, it may not be a good idea to assume that q begins with the exact string q' because there are so many variations in expressing the query (read song title) q that in the past the exact string might never have been encountered. Thus, a query log based query completion technique that is agnostic to spelling variations will require a huge amount of past queries on song titles to achieve an accuracy comparable to that for usual queries. In this work, we describe a new query completion technique for Bollywood1 song title search. The method does not use query logs, but only a list of song titles crawled from the Web. It uses a transliteration engine, Microsoft Indian Language Input Tool (MSILIT) [4], which generates possible Devanagari equivalents for an input string in Roman script. This allows the user to type in a Bollywood song title query in Roman script. While the user is typing, the system dynamically suggests up to 10 complete song titles that are most relevant for the partially typed query. For song search, our system outperforms the query completion feature of the most popular search engines, even though it does not have access to any query log.Categories and Subject DescriptorsH.3.3 [Information Search and Retrieval]: Query FormulationGeneral TermsAlgorithms, Measurement, ExperimentationKeywordsQuery completion, Transliteration, Song search, Query log1. INTRODUCTIONSong titles and lyrics are one of the most popular categories for web-search. Songs in languages using non-Roman scripts, such as Hindi, Arabic and Bengali, pose a challenge as the titles and lyrics are commonly searched and retrieved in their Roman transliterations. In many cases, no standard transliterations exist leading to a number of valid spelling variations for each word [1]. For example, hai, hain, hey, he and hay could refer to the same Hindi word, leading to several variations of the Hindi song title "dil to pagal hai". Since the other words in this title will also have quite a few commonly used Roman spellings, one ends up with a large number of possible variations for the song title. As the patterns and extent of variations in song titles are very different from usual spelling errors, IR techniques such as spelling correction, query suggestion and completion require specialized methods for these queries. Query completion helps users by suggesting appropriate complete queries based on partially typed strings. Modern search engines primarily rely on query logs for query completion [2,3]. The techniques are based on the premise that if a user has typed a partial query q', then it is very likely that he/she is trying to formulate a query q which is one of the most frequent queries in2. METHODGiven a partial query q' = w1w2...wk, where w1, w2 etc. are Roman transliterations of Hindi words (wk could be a partially typed word), we first generate a list of candidate song titles that the user might be searching for, compute a relevance score of each song title against q' and finally output the top 10 (or less) titles displayed in decreasing order of their relevance scores.2.1 Candidate GenerationThe candidate generation algorithm relies on the following two observations: (a) Even though the Roman transliterations of the titles have a large number of valid variations, in Devanagari script usually there is only one correct spelling for the titles (e.g., " " for "dil/dhil to/toh pagal/paagal hai/hay/he"); (b) while searching for song titles, people usually start from the first word and type words in correct sequence. We use the MSILIT tool to generate up to 5 possible Devanagari variations of the wi's. Let us denote these variations for wi as di1, di2... di5. Next, we generate possible Devanagari transliterations of q' by combining the variations of individual words (e.g., d11d21...dk1, d12d21...dk1 and so on). Thus, we have at most 5k combinations. Since, in practice k is small (usually between 1 and 3, because if the query completion system is able to output theCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.1Bollywood is the informal term popularly used for the Hindilanguage film industry based in Mumbai, India. (Wikipedia)31WWW 2011 ­ Poster intended query, it does so within typing of the first 3 words), the number of combinations generated is of the order of few hundreds, and therefore, tractable. These Devanagari strings are then searched in a database of song titles in Devanagari. If no matches are returned, we search for song titles which have all the words of a string, but not necessarily in exact order. If even this fails to return any match, we resort to titles which have as many words of the string as possible. For each of the song titles returned by this approach, we select one of its more common Roman transliterations as a candidate for suggestion. If the above method does not return any match, we search for q' in a song title database in Roman script. The databases of song titles have been built by crawling 15 popular websites for Bollywood lyrics in either Devanagari or Roman or both. The websites containing lyrics in both the scripts were used to align the song titles in the two scripts. There are about 50000 unique song titles in the database.March 28­April 1, 2011, Hyderabad, India Table 1. Recall statistics of the system (in %)q' w1 w1 w2 w1 w2 w3 w1w2w3w4 Rank 1 18.0 57.1 69.0 66.7 Within Rank 2 26.0 66.2 72.3 68.0 Within Rank 3 34.0 69.3 73.5 69.4 Within Rank 10 46.0 79.6 79.3 69.4 Not found 54 17 10 9which the correct suggestion was not generated for any partial query up to a specific length. Users usually select the correct suggestion as soon as it is presented for the first time. Thus, the number presented in the last column provides an estimate of the number of words that a user need to type. We also evaluated the query completion feature of three of the most popular commercial search engines on this data set. While our system outperforms all these search engines by a large margin for q' = w1, for longer partial queries it is comparable to the best of the three engines and significantly better than the other two. These commercial search engines have access to huge query logs and a much larger part of the Web. Therefore, it is fair to assume that with access to similar resources, the current system will by far outperform the existing search engines. Of course, one should also keep in mind that we do have the unfair advantage of working on a very specific domain, that is, Bollywood song titles.2.2 RankingFor every candidate, we compute its static and dynamic scores. The static score tries to capture the general popularity of a song title based on the following two features: the number of crawled websites featuring this title (the higher the better), and the release date of the song (the newer the better). Some websites provide popularity rating for songs, but we found these scores to be very sparse and unreliable. Another excellent indicator of popularity is the frequency with which a song is searched for, but we do not have access to any sizeable query log in this domain to estimate this feature. The dynamic score indicates the relevance of a candidate in the context of the partial query q'. If we have candidates with exact string match, this score is defined as 10 ­ p, where p is the positional index of the first word of the candidate which can be aligned to w1 of q'. The dynamic scores for the cases where an exact string match is not found are computed in a similar manner. The relevance score is computed by adding the static and dynamic scores (computed on scales of 0-15 and 0-10).4. CONCLUSIONS AND FUTURE WORKHere we described a new method for query completion for Bollywood song search. It is robust to spelling variations and does not use any query log. The technique is scalable to any sufficiently restrictive domain where the queries are transliterations of native words in a non-native script (usually Roman). Examples of similar scenarios include song, movie and book title search for languages which do not use Roman script, but Roman transliterations are fairly common (e.g., Arabic, Chinese, Japanese and most of the Indian languages). The only language dependent components of our system are the transliteration engine and rules for generating spelling variations in Roman script. The performance of the current system can be boosted by (a) crawling more websites, (b) using query logs, (c) fine tuning the relevance score formula using machine learning techniques, and (d) improving the performance of the transliteration engine. Currently, we are working on all these aspects.2.3 Implementation StrategyDue to repeated calls to the MSILIT engine and several database searches, generation and ranking of the candidates during runtime takes up to a few seconds. This is not desirable for a query completion system. We circumvent this problem as follows: For each unique song title, we guess all possible Roman variations and construct a prefix trie for all variations of all titles; then for every node of the trie, we run the above algorithm offline and compute the list of up to 10 most relevant song titles for the query string that leads from the root to that node. This list is stored in the trie node, so that the online computation only involves traversing the trie. The Roman variations for the Hindi words are constructed through a rule-based approach that has high recall, but low precision. MSILIT is then run on the generated strings to prune the invalid variations.3. EVALUATIONWe evaluated the system on 100 song title queries that were collected from 20 users who are familiar with the domain and frequently search for Bollywood songs. For each query, we generated 4 partial queries by considering up to the first 4 words and checked whether the system was able to generate the correct suggestion, i.e., the intended song title; and if so, then at what rank. The aggregate results are presented in Table 1. The last column of the table reports the number of queries (out of 100) for32
13	web_mining web_information_integration	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaHyLiEn: A Hybrid Approach to General List Extraction on the WebFabio Fumarola Tim Weninger Rick Barber Donato Malerba Jiawei HanUniversity of Illinois at Urbana-Champaign Università degli Studi di Bari "Aldo Moro"ffumarola@di.uniba.it, weninge1@illinois.edu, barber5@illinois.edu, malerba@di.uniba.it, hanj@illinois.edu ABSTRACTWe consider the problem of automatically extracting general lists from the web. Existing approaches are mostly dependent upon either the underlying HTML markup or the visual structure of the Web page. We present HyLiEn an unsupervised, Hybrid approach for automatic List discovery and Extraction on the Web. It employs general assumptions about the visual rendering of lists, and the structural representation of items contained in them. We show that our method significantly outperforms existing methods.A1_1_1 A1_1_2 A1_1_3 A1_1_4 A1_1_4A1_1 A1_2 A1A1_3_1A1_3_2A1_3_3A1_3_4A1_3_5A1_3_6A1_3A2_1 A2_2_1 A2_1_1 A2_1_2 A2_1_3 A2_1_4 A2_1_5 A2_1_6 A2_3_1b A2_3_2b A2_3_3b A2_2_1_1 A2_2_1_2 A2_2_1_3 A2_2_1_4 A2_2_1_5 A2_2_2 A2_2_2_1 A2_2_2_2 A_2_2_3 A2_2_2_4 A2_2_2_5 A2_2_3 A2_2_3_1 A2_2_3_2 A2_2_3_3 A2_2_3_4 A2_2_3_5 A2_2_3_6 A2_2 A2_3 A2 A2_3_4a A2_3_1a A2_3_2a A2_3_3aA3_1_1A3_1_2A3_1_3A3_1_4A3_1 A3Categories and Subject DescriptorsH.2.8 [Database Management]: Database applications-- data mining; H.3.1 [Content Analysis and Indexing]: [structured data extraction] (a)A3_2_1A3_2_2A3_2_3A3_2_4A3_2(b)KeywordsWeb lists, Web mining, Web information integrationFigure 1: The Illinois Mathematics Web Page and its box structure In this paper, we propose HyLiEn, a novel hybrid based approach for automatic discovery and extraction of general lists on the Web. Although there are already some works that pay attention on visual information on Web pages [2, 4] and on the DOM-structure of a Web page [6, 1, 7], to the best of our knowledge, only a few methods use both visual and DOM information to perform Web list discovery [2, 5] the most recent of which, VENTex [2], is used in our case study for comparison.1.INTRODUCTIONThis work focuses on extracting information from lists on the Web. Lists are interesting because they present information in a condensed, well structured way. The characteristics of Web lists vary widely. Consequently, a great variety of computational approaches have been applied to discover and extract the information embedded in lists. These existing approaches mostly rely on the underlying HTML markup and corresponding DOM structure of a Web page [6, 1, 7]. Unfortunately, HTML was initially designed for rendering purposes and not for information structuring (like XML). As a result, a list can be rendered in several ways in HTML, and it is difficult to find an HTML-only tool that is sufficiently robust to extract general lists from the Web. Visual information extraction approaches move the focus of the problem from the HTML and its corresponding DOM tree structure to a visual pattern recognition problem [2, 4]. These visual-based methods are still inadequate to be used for general list extraction. They tend to focus on subproblems, such as the extraction of tables where each data record contains a link to a detail page [3], or discovering tables rendered from Web databases [4] (deep web pages).Copyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0632-4/11/03.2.VISUAL-STRUCTURAL METHODHyLiEn is primarily based on the visual alignment of list items, but it also utilizes non-visual information such as the DOM structure and the size of visually aligned items. The result of the Web page rendering process can be regarded as a set of boxes. Figure 1(a) and 1(b) show an example of web page and box segmentated page. Each rendered box has a position and size, and can either contain content (i.e., text or images) or more boxes. Generally, there is a box created for each DOM element. Starting from the box representing the entire Web page, usually the HTML tag, we recursively consider inner boxes, and extract lists boxes which are visually aligned and structurally similar to other boxes. To extract lists on the basis of visual cues, the following basic assumption is made: Definition 1. A list candidate l = {l1 , l2 , . . . , ln } on a ren-35WWW 2011 ­ Posterdered Web page consists of a set of vertically and/or horizontally aligned boxes. As shown in [8], this assumption alone is sufficient to outperform all existing list extraction methods. However, it does not cover Web pages such as the one in Figure 1(a) where, all of the orange boxes inside Box A2 2 correspond to a single list in the page, but there are many pairs of elements in this list which are not visually aligned. Therefore, we extend the assumption in definition 1 as following: Definition 2. Two lists l and l are related (l l ) if they have an element in common. A set of lists S is a tiled structure if for every list l S there exists at least one other list l S such that l l and l = l . Lists in a tiled structure are called tiled lists. Three tiled lists (A2 2 1 , A2 2 2 and A2 2 3 ) are shown in Figure 1(b). The notion of tiled list is useful to handle more problematic cases, such as A2 2 , by merging the individual lists of a tiled structure into a single tiled list. Once visual aligned elements are extracted, we prune out false positives under the hypotheses that the DOM-subtrees corresponding to the elements of the list must satisfy a structural similarity measure (structSim) to within a certain threshold and that the subtrees not have a number of DOM-nodes (numN odes) greater than a threshold : Definition 3. A candidate list l = {l1 , l2 , . . . , ln } is a genuine list if and only if for each pair (li , lj ), i = j, structSim (li , lj ) , numN odes(li ) , and numN odes(lj ) . This assumption, which is shared with most other DOMcentric mining algorithms, is made to determine whether the visual alignment of a certain boxes can be regarded as a real list or it should be discarded.March 28­April 1, 2011, Hyderabad, IndiaRecall 85.7% 99.7% Precision 78.0% 99.9% F-Measure 81.1% 99.4%VENTex HyLiEnTable 2: Precision and Recall for record extraction on the VENTex data set.4.CONCLUSIONSIn this paper, a novel fully automatic hybrid approach for extracting Web lists from Web pages is presented. Experimental results show that it outperforms the existing approaches for Web lists and tables extraction. Interesting avenues for future work involve the usage of extracted lists to annotate and discover relationships between entities on the Web or to index the Web, the query answering from lists, and the entity discovery and disambiguation using lists.5.ACKNOWLEDGMENTSThe first and fourth authors are supported by the Project DIPIS funded by Apulia Region. The second and fifth authors are supported by NSF IIS-09-05215, U.S. Air Force Office of Scientific Research MURI award FA9550-08-1-0265, and by the U.S. Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053 (NS-CTA).6.EXPERIMENTSHyLiEn requires two parameters which are empirically set to = 0.6 and = 50. We used a subset of 100 pages from dataset used in VENTex[2]. This dataset is general and not biased; we take advantage on this dataset to show that our method is robust and the result are not biased from the selected test set. Ground truth 224 6146 VENTex 82.6% 85.7% HyLiEn 79.5% 99.7%# tables # recordsTable 1: Recall for table and record extraction on the VENTex data set. Table 1 shows that there are over 224 tables and 6146 data records in the ground truth.VENTex did extract 8 more tables than HyLiEn. We believe this is because HyLiEn does not have any notion of element distance that could be used to separate aligned but separated lists. Despite the similar table extraction performance, HyLiEn extracted many more records (i.e., rows) from these tables than VENTex. We did judge the precision score here for comparison sake (see Table 2). We find that, from among the 100 Web pages we achieve 99.9% precision. VENTex remained competitive with a precision of 85.7%. We see that HyLiEn consistently and convincingly outperforms VENTex.[1] M. J. Cafarella, A. Halevy, D. Z. Wang, E. Wu, and Y. Zhang. Webtables: exploring the power of tables on the web. Proc. VLDB Endow., 1(1):538­549, 2008. [2] W. Gatterbauer, P. Bohunsky, M. Herzog, B. Kr¨pl, u and B. Pollak. Towards domain-independent information extraction from web tables. In WWW, pages 71­80, New York, NY, USA, 2007. ACM. [3] K. Lerman, L. Getoor, S. Minton, and C. Knoblock. Using the structure of web sites for automatic segmentation of tables. In SIGMOD, pages 119­130, New York, NY, USA, 2004. ACM. [4] W. Liu, X. Meng, and W. Meng. Vide: A vision-based approach for deep web data extraction. IEEE Trans. on Knowl. and Data Eng., 22(3):447­460, 2010. [5] K. Simon and G. Lausen. Viper: augmenting automatic information extraction with visual perceptions. In CIKM, pages 381­388, New York, NY, USA, 2005. ACM. [6] S. Tong and J. Dean. System and methods for automatically creating lists. In US Patent: 7350187, Mar 2008. [7] R. C. Wang and W. W. Cohen. Language-independent set expansion of named entities using the web. In ICDM '07: Proceedings of the 2007 Seventh IEEE International Conference on Data Mining, pages 342­350, Washington, DC, USA, 2007. IEEE. [8] T. Weninger, F. Fumarola, R. Barber, J. Han, and D. Malerba. Unexpected results in automatic list extraction on the web. SIGKDD Explorations, 12(2), 2010.36
14	context content	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaWonderWhat: Real-time Event Determination from PhotosMingyan GaoXian-Sheng HuaMicrosoft Research, Asia Beijing, ChinaRamesh JainSchool of Information and Computer Science University of California, IrvineSchool of Information and Computer Science University of California, Irvinegaom@ics.uci.eduxshua@microsoft.comjain@ics.uci.eduABSTRACTHow often did you feel disappointed in a foreign country, when you had been craving for participating in authentic native events but miserably ended up with being lost in the crowd, due to ignorance of the local culture? Have you ever imagined that with merely a simple click, a tool can identify the events that are right in front of you? As a step in this direction, in this paper, we propose a system that provides users with information of the public events that they are attending by analyzing in real time their photos taken at the event, leveraging both spatio-temporal context and photo content. To fulfill the task, we designed the system to collect event information, maintain dedicated event database, build photo content model for event types, and rank the final results. Extensive experiments were conducted to prove the effectiveness of each component.been advanced to bridge the gap , but the languages and query systems designed so far are still very "expert-oriented" [2] and not usable for common users. Visual search has gained increasing popularity and worked successfully for certain objects and text[3]. However, due to the semantic gap, the approach is insufficient for concepts with richer semantic meaning, like event.1. Event 3 Movie Premiere Twilight Saga: Eclipse Jun. 25th 2010 Kodak Theater more >> 2. Event 1 ... 3. Event 2 ...RankingQuery (image, time, location)Event Type Image Content-based AnalysisContent Analysis and RankingCategories and Subject DescriptorsH.4 [Information Systems Applications]: MiscellaneousEvent DatabaseCrawling and Extractionevent1, event2, event3 Spatio-Temporal QueryGeneral TermsDesign, Experimentation, Human FactorsEmergent Event Detection Tweet Tweet TweetKeywordsEvent Determination, Photo, Context, Content1.INTRODUCTIONWhen we are traveling in foreign countries, it is often a big problem for us to understand the events or activities that are going on in the local area. The difficulty prohibits common tourists from better appreciating different places and cultures. Can we create a tool that is capable of determining these events and delivering information about them in real time? Efforts have been made on utilizing the rich data on the web to provide knowledge to end users. However, no existing approaches can fully satisfy our need. Traditional web search engines rely on keyword indexing. Although many techniques have been proposed to better understand user intentions in search, the gap between concepts and keywords still exists. Work on semantic web and related search have This work was performed when the author was visiting Microsoft Research Asia as a research intern.Copyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.Figure 1: System Architecture In this work, we propose a system that provides users with information of the public events that they are attending by analyzing in real time their photos taken at the event. Whenever a user wants to know about an event she is currently at, she only needs to take a picture of it. By examining the photo content together with the spatial and temporal data carried with it, our system automatically returns a ranked list of events with which the photo may be associated. Our approach has the following advantages. 1) Use of the system is very intuitive and requires no special efforts; 2) The system keeps a dedicated event database and index, and automatically constructs queries for users, which enables the delivery of exact event information; 3) Our system not only detects planned events, but also tries to discover concurrent events by analyzing real-time micro-blogs; 4) Different types of events do reveal distinct visual characteristics, so visual content is also taken into account to improve search results. As far as we know, there is no previous work that has addressed a similar problem.2. OUR APPROACHProblem We define a contextual photo as p = (img, time, location), where img is the image content of the photo p,37WWW 2011 ­ Posterand time and location = (latitude, longitude) correspond to the timestamp and geo-coordinate of the photo p. We model event as e = (time, location, title, description, type, media). time = (start, end) defines the time interval during which e occurs. location = (lat1 , long1 , lat2 , long2 ) represents the geo-coordinates of the southwest and northeast corner of the place where e takes place. Name of the event e is stored in title, and the textual explanation to e is saved in description. Event types, such as performances, exhibitions, sports are stored in type. Media data, e.g. posters and photos, is kept in media attribute. Given an incoming contextual photo p, our problem is to compute a ranked list of events (e1 , ..., en ), which are in decreasing order of likelihood that ei is the event at which the photo p was taken. Approach We present the system architecture in Figure 1. Our system consists of the following major steps. 1. We create an event database, and ingest both planned and emergent events into it. Planned events, which are usually pre-declared online, are extracted from web pages or downloaded via web services that perform event integration. Emergent events, the occurrence of which are impromptu, are detected from Twitter by following the approach in [1]. 2. After an user takes a photo for an event, her capturing device creates a contextual photo containing the image content, time and location information. The device then sends this contextual photo to our system as a query. In our system, 3. First, given the time and location information in the contextual photo, a query is issued to the event database, which returns a list of related events. 4. Then the content analysis component analyzes the image content and returns the event type of the event captured in the photo. In this work, we model the relationship between event types and the raw visual features through a middle layer of visual concepts. We employed a learning based approach to perform the analysis, which consists of four major steps: 1) Train concept detector; 2) Detect concepts from photos associated with different event types; 3) Train event type detector; 4) For each incoming photo, based on the models, decide which type of event the photo is most likely to be. 5. Both the event list from event database and the detected event type are given to the ranking component. The component considers spatial, temporal, and visual distances in the final ranking process. 6. Finally, a ranked list of events and their associated information are returned to the user and presented on her device.March 28­April 1, 2011, Hyderabad, IndiaEvents Title: Phantom of Opera Time: 2008-03-15 14:00:00 Location: Majestic Theater Type: Shows http://newyork.metromix.com/browse/events?date_type =today&date_range=2008-03-15%2C2008-03-15 Title: Playing the Building: An Installation by David Byrne Time: 2008-06-28 12:00:00 - 18:00:00 Location: 10 South Street, Manhattan Type: Exhibitions http://www.nyc.gov/portal/site/nycgov/menuitem.bd175 b51da17d74f472ae1852f8089a0/index.jsp?&epicontent=GENERIC&epiprocess=generic_process.jsp&beanID=1622248501&viewI D=calendar_process_view&pageop=eventbrowser&range =day&selectedDate=6/28/2008&p=100 Title: Heritage of Pride Parade Time: 2008-06-29 12:00:00 ­ 18:00:00 Location: Fifth Ave - Greenwich Street - to Christopher St., Manhattan Type: Parades http://www.nyc.gov/portal/site/nycgov/menuitem.bd175 b51da17d74f472ae1852f8089a0/index.jsp?&epicontent=GENERIC&epiprocess=generic_process.jsp&beanID=1622248501&viewI D=calendar_process_view&pageop=eventbrowser&range =day&selectedDate=6/29/2008&p=50 Title: New York Yankees vs. Boston Red Sox Time: 2009-05-05 19:05:00 Location: Yankee Stadium Type: Sports http://new.york.eventguide.com/days/90505.htm PhotosFigure 2: Examples of matched events and photos. sets collected from 4 volunteers living in NYC. We asked each volunteer to hang around on streets in NYC during their spare time, in August and September 2010, and try attending some events that they discovered. They were advised to take as many pictures as possible at the event, and there were no requirements on the subjects of these photos. The ranking result is depicted in Figure 3. The photo column shows a sample of pictures from each photo set, and the result column lists the top 5 ranking result for most pictures in the photo set. The last column provides the ground truth of these events. For event 1, 3, and 4, we correctly returned the information of the corresponding events on the first place in ranked lists. But for event 2, since the exact event was not stored in our database, our system returned a musical event in the Mitzi Newhouse Theater of Lincoln Center, which was a very close match.Photos 1Ranking Result 1. Free Music Fridays 2. Target Free Fridays 3. Film Collection at MoMA 4. Drawings Collection at MoMA 5. Contemporary Art from the Collection 1. The Grand Manner Ground Truth Title: Free Music Fridays Time: 09/10/2010 5:30PM ­ 7:30PM Location: American Folk Museum Type: Museum, Performance Title: Metropolitan Opera's HD Festival Time: 09/01/2010 8:00PM Location: Lincoln Center Type: Performance Title: River-to-River Festival Visiting Governors Island Time: 08/29/2010 10:00AM Location: Governors Island Type: Festival/Fair231. River-to-River Festival41. Street Fairs 2. Improv 4 Kids! 3. Tony n Tinas Wedding 4. Chicago 5. The Quantum Eye - Magic DeceptionsTitle: Street Fairs Time: 08/21/2010 10:00AM th Location: On 6 Avenue from nd th 42 ­ 56 Street Type: Festival/Fair3.EXPERIMENTSFigure 3: Result on real photo sets.We conducted experiments on both Flickr dataset and a real event photo set shot in New York City. Flickr Dataset In this experiment, we verify the hypothesis that people do take photos at events. And by making use of the taking time and location of photos, we are able to match them to the corresponding events. We built the event database for events in NYC from year 2008 to 2010. Also, we called the Flickr API and downloaded all the photos shot from year 2008 till June 2010. We matched the photos to the events in the event database. Figure 2 shows examples of matched events and photos. The left column details the events and the urls where these events were extracted, and the right column lists the photos taken at the events. Real Photo set In this section, we test on real photo38
16	online_social_networks twitter	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaSpammers' Networks within Online Social Networks: A Case-Study on TwitterSaptarshi Ghosh Gautam Korlam Niloy GangulyDepartment of CSE, Indian Institute of Technology Kharagpur, India{saptarshi,gautam,niloy}@cse.iitkgp.ernet.inABSTRACTWe analyze the strategies employed by contemporary spammers in Online Social Networks (OSNs) by identifying a set of spam-accounts in Twitter and monitoring their linkcreation strategies. Our analysis reveals that spammers adopt intelligent `collaborative' strategies of link-formation to avoid detection and to increase the reach of their generated spam, such as forming `spam-farms' and creating large number of links with targeted legitimate users. The observations are verified through the analysis of a giant `spam-farm' embedded within the Twitter OSN.Categories and Subject DescriptorsH.3.5 [Online Information Services]: Web-based services; J.4 [Computer Applications]: Social and behavioral sciencesGeneral TermsExperimentation, SecurityKeywordsOnline Social Networks, Twitter, spam-farms1.INTRODUCTIONployed by spammers in the OSNs; knowledge of these strategies can be utilized for designing effective and proactive spam-control techniques. Here we demonstrate that analysis of the social link-creation patterns of spammers can provide valuable insights into the spammers' strategies. A primary objective of spammers in OSNs is to acquire a large number of social links or friends (`followers' in Twitter1 ), since (i) this helps them pose as popular users and thus evade suspicion of spam-detection algorithms, and more importantly, (ii) this enables rapid dissemination of spam to a large audience using the one-to-many communication methods provided in OSNs (e.g. public `tweets' multicast to all followers in Twitter). In most OSNs, one-to-all-friends modes of communication are reserved for use across existing social links only, hence spammers aim to acquire a large number of social connections (followers in Twitter) and then use these modes of communication. What strategies do spammers employ in present-day OSNs to attain this objective? Spammers in the Web are known to change the link structure of their affiliate web-sites to create `link-farms' in order to deceive search engines and thus improve the ranking of one or more of these web-pages [1]. Do spammers in OSNs adopt a similar technique of linking to one another to create their own `spam-farms' in order to pose as popular users? We conducted several empirical experiments on the Twitter OSN to answer these questions, as are described below.Popular OSNs (e.g. Twitter, Facebook) of today are being increasingly targeted by spammers and other malicious users to promote affiliate websites and disseminate malware. Though many techniques for controlling malicious users have been proposed, such as machine-learning based techniques [4], Sybil-defense schemes [5], and so on, large amounts of spam continue to plague the popular OSNs. This is most probably because the OSN authorities are unwilling to deploy automated methods at large scale to detect suspicious useraccounts and suspend them, fearing that wrong decisions would lead to serious discontentment among users of the OSN. Hence spam-accounts are suspended mostly after a sufficient number of users report them as spam. However, most legitimate users are unwilling to invest time in reporting against spammers, hence spammers are being allowed more time to spread spam in the OSNs. In our ongoing research, we are investigating methods to overcome this problem by understanding the strategies emCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.2.ANALYZING SPAMMERS' STRATEGIESWe started with 8 spam-accounts in Twitter (obtained heuristically from a large crawl of Twitter users studied by us in [2] and verified manually) and crawled their neighbourhood to detect other suspicious user-accounts. As reported in [4], spammers in Twitter repeatedly post tweets containing URLs of their affiliate web-sites, hence we use the number of repeated URLs in recent tweets to identify suspicious users. Note that this feature is not a confirmatory test for spammers, since some promoters also repeatedly post the same URL to advertise their websites or services [4]. We ran selective BFS crawls starting from the 8 spammers in August 2010; among the followers and followings of the user currently being crawled, only the suspected ones were added to the BFS queue to be crawled subsequently. We found 3471 suspicious user-accounts within a distance of two1 Twitter is a directed social network where user u `follows' user v if u intends to receive all `tweets' (messages) posted by v. In Twitter terminology, u is a `follower' of v and v is a `following' of u.41WWW 2011 ­ Poster`hops' from the set of 8 spammers; out of these, 79 have been suspended by Twitter authorities by November 10, 2010. Since active accounts are suspended by Twitter primarily for spam-activity, we assume that accounts that are suspended were actually spam-accounts. Surprisingly, out of these 79 spam-accounts, many had several thousand followers (and followings) not only from other spammers, but also from thousands of legitimate users in the OSN. How did the spammers acquire so many followers from among legitimate users? We observed that there exist many user-profiles which are followed by (and follow) several of the spammers. To gain a detailed insight into the strategies of spammers, we filtered out the top 50 accounts based on the number of spam-accounts connected with them (out of the 79). Of these 50, 5 were already suspended by Twitter, showing that they were spammers as well. We created a Twitter account and followed the other 45. Within a single day, our Twitter account acquired 33 followers - some were reciprocating follows by the users whom we followed, the others were unsolicited follows by users who were themselves suspicious. This experiment demonstrates that (i) spammers not only follow other spammers, but also target specific legitimate users who frequently follow back (mostly promoters of some service, who consider it a social etiquette to follow back prospective customers) (ii) spammers monitor the followers of the targeted legitimate users, and begin to follow those who follow the targeted users, hoping to get followed back; possibly this is how spammers discover other potential spammers and collaborate with them (e.g. in growing the set of legitimate users to target).March 28­April 1, 2011, Hyderabad, Indiafraction of reciprocal followings1 0.8 0.6 0.4 0.2 0 103 104 105 number of followingsFigure 1: Variation of fraction of reciprocated follow-links with number of follow-links created by spammers confirms that spammers selectively follow users who followback. Moreover, spammers having higher number of followings have higher reciprocation on the average as well, showing that as spammers spend more time in the network and create more links, they are able to filter out more users who follow-back and thus expand the reach of the spam. (iv) Large overlaps exist among the neighbours of the spammers (e.g. 275 accounts are followed by more than 1000 of the 4491, and 345 accounts follow more than 1000 of the 4491 spammers). implying large-scale collaboration among spammers in identifying potential users to follow.4.DISCUSSION AND CONCLUSION3.LARGE-SCALE ANALYSIS OF SPAMMERSTo verify our findings, we consider a publicly available snapshot of the Twitter OSN as in July 2009 [3]. We attempted to crawl the profile-pages of a large fraction of the 41.7 million users in the snapshot, and discovered more than 30,000 user-accounts that have been suspended by Twitter since July 2009 2 . Out of these, we studied the 4491 accounts that had more than 1000 each of followers and followings; this large number of social links created show that these accounts had been active ones, hence they were suspended most probably due to spam-activity. Analyzing the social links created by these spam-accounts, we observe the following: (i) The 4491 spam-accounts had more than 730,000 directed links among them, confirming the presence of a giant spamfarm having a density of 0.036, several orders of magnitude higher than the density of 8.46 × 10-7 for the entire Twitter snapshot. In fact, it is intriguing that spam-accounts are able to find (and link to) other spam-accounts so frequently, within a social network of the size of Twitter. (ii) On an average, 4.74% of the follow-links created by these spammers lead to other spammers (within the 4491), and this fraction is as high as 12% for some of the accounts (iii) Fig. 1 plots the fraction of reciprocated follow-edges of these spammers against the number of follow-edges created. Compared to the reported 22.1% reciprocation for the entire Twitter snapshot [3], almost all spammers have much higher reciprocation of the follow-edges they create. This2 attempts to crawl the profile-page of these lead to a Twitter web-page showing that the account has been suspendedWe uncover several tactics employed by spammers in OSNs: not only do spammers collaborate among themselves by forming giant `spam-farms', they also target many specific legitimate users who unwittingly create links with spammers. As a result, though the spammers form dense communities among themselves, such communities get deeply embedded into the social network and become extremely difficult to identify. This can lead to large-scale Sybil-attacks in distributed OSNs, and the Sybil-defense schemes of today which assume that although a malicious Sybil-attacker can create an arbitrary number of sybil-nodes in the social network, such sybil-nodes can only form a limited number of social links to legitimate (non-Sybil) nodes [5] - are likely to face new challenges in dealing with large spam-farms having millions of links with legitimate users. Thus we have identified the footprints left by large spamfarms within OSNs, and provided several insights on the link-creation strategies of spammers, that need to be considered while developing anti-spam strategies. For instance, proactive strategies can be tried, such as automatic monitoring of suspicious users in the neighbourhood of reported spammers, and warning legitimate users against reciprocating to `friend requests' from the monitored suspicious users.5.42
19	trust clustering	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaTrust Analysis with ClusteringManish Gupta Yizhou Sun Jiawei HanUniversity of Illinois at Urbana Champaign{gupta58,sun22, hanj}@illinois.edu ABSTRACTWeb provides rich information about a variety of objects. Trustability is a major concern on the web. Truth establishment is an important task so as to provide the right information to the user from the most trustworthy source. Trustworthiness of information provider and the confidence of the facts it provides are inter-dependent on each other and hence can be expressed iteratively in terms of each other. However, a single information provider may not be the most trustworthy for all kinds of information. Every information provider has its own area of competence where it can perform better than others. We derive a model that can evaluate trustability on objects and information providers based on clusters (groups). We propose a method which groups the set of objects for which similar set of providers provide "good" facts, and provides better accuracy in addition to high quality object clusters. provides the right information for all the objects (books), which may not be true. It might be good to propagate trust information of a provider to recompute the confidence of facts of only those objects for which the provider is considerably trustworthy. The problem is how to define clusters of objects such that those objects for which a group of top providers have similar trust are grouped together.3. THE ITERATIVE FACT FINDER MODELBasic truth finder [4] provides a model to compute global trustworthiness of providers and ranks facts associated with the objects based on their confidence. Trustworthiness of provider p is t(p) and s(f ) is confidence of the fact f . Each fact is associated with an object. Optionally, implication from fact f1 to fact f2 (imp(f1 f2 )) denotes influence of fact f1 on fact f2 . Let P (f ) denote the set of providers that publish fact f and F (p) denote the set of facts provided by provider p. Pasternack et al. [2] introduce a few more fact finders (Sums, Average.Log, Investment) which are based on the same framework as truth finder [4], but differ in the way the confidence and trust values are computed. Figure 1 provides an example that shows why cluster based ranking of providers can be different from global ranking and how it can be useful for computing fact confidences. Providers p1 and p2 provide facts fij (ith provider, j th object) for five objects o1 to o5 . p1 provides good facts for objects o1 and o2 (and bad facts for o3 and o4 ) while p2 provides good facts for the other three objects (and bad facts for o1 and o2 ). Since p2 provides good facts for more objects, most fact finders would rank p2 higher than p1 . But if we look at the trust profiles of the objects in the provider space, we notice that objects o1 and o2 have a similar profile while the objects o3 , o4 , o5 have similar profiles. Thus, we can cluster objects into two clusters. We notice that for cluster c1 , p1 would be ranked higher than p2 and vice versa for cluster c2 . Thus, if we cluster objects in the providerGlobal ranking f11 f21 f12 Cluster based ranking1Categories and Subject DescriptorsH.3.3 [Information Search and Retrieval]: Information filtering; H.2.8 [Information Systems Applications]: Database Applications--Data miningGeneral TermsAlgorithms, Experimentation, MeasurementKeywordstrust, fact finding, clustering1. INTRODUCTIONLarge amounts of structured information are available on the web. Consider the set of author lists for different books available on book selling websites. First there is a need to establish the trustworthiness of each of the websites across all the different books and then there is a need to cluster the books according to the similarity between the sets of "good" websites for each of the books. Note that this clustering is based on trustworthiness and may be quite different from natural clustering on a single dimension.2. MOTIVATION AND RELATED WORKIn presence of conflicting time-varying information provided by a large number of possibly dependent sources, voting may not be the best method for veracity analysis. Yin et al. [4] presented the basic truth finder model which aimed at finding true facts from a large amount of conflicting information on many subjects that is provided by various web sites. Dong et al. [1] studied the problem of finding true values and determining the copying relationship between sources, when the update history of the sources is known. We perform a clustering and ranking of websites and objects iteratively. The closest related clustering work is RankClus [3]. We use a similar philosophy for the design of our algorithm. In [4], provider trust depends on confidence of facts (author lists) published by that provider, while confidence of a fact depends on trustworthiness of the providers that publish that fact and confidence of other related facts. They compute the confidence of the facts and the overall trustworthiness rankings of the providers iteratively in terms of each other. Assumption is that a trustworthy provider (website)Copyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.o1 o2 o3 o4 o5t(p) p2 p1p1 p2 tc (p) p1 p2 c1 tc (p) p2 p12p1 p2f22 f13 f23 f14 f24 f25c2Figure 1: Trustworthiness information propagation should be restrictive trustworthiness space, we may derive interesting clusters and thereby improve trust and confidence computations. Note that an object can belong to only one cluster, but a provider may be an expert for multiple clusters of objects.4.OUR APPROACHWe want to do trust analysis based on trustworthiness of providers and confidence of facts related to the objects, and obtain clustering of objects. We hypothesize that objects can be clustered based on provider trustworthiness profiles (to (p)) personalized to the particular object. Also, restrictive flow of trust information across objects, using clusters, can improve ranking accuracy of facts and providers.53WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaTable 1: Accuracy (TF=Truth Finder [4], AL=Average Log [2], Inv=Investment [2])books-orig population bdate ddate Voting Basic BCFF ACFF Voting Basic BCFF ACFF Voting Basic BCFF ACFF Voting Basic BCFF ACFF TF 87.676 93.495 93.162 93.495 95.494 97.077 97.045 96.987 89.261 82.261 82.261 82.261 92.85796.065 95.341 96.065 Sums 87.676 85.043 86.293 85.210 95.494 97.534 97.435 97.53489.26190.91386.609 86.609 92.857 93.746 95.522 93.971 AL 87.676 86.970 86.937 87.520 95.494 96.710 97.273 96.958 89.261 90.913 86.60990.95792.857 93.261 95.116 95.522 Inv 87.676 88.192 88.551 88.151 95.494 97.436 97.530 97.57389.26190.95786.60990.95792.857 94.123 95.116 95.312If we know some natural clustering C = {ck }K , we can k=1 run basic fact finder model for each of the clusters of objects separately. But, we would not use the information about providers related to objects in other clusters. Also, this method needs some input clustering. Clusters are fixed and depend on a particular dimension only. Alternatively, we can compute provider trust per object and then cluster the objects using their object-conditional trust vectors (to (p)) as shown in Algorithm 1. to (p) is computed as the confidence of the fact provided by provider p for the object o. It denotes the trust of information provider p conditioned with respect to the object o. Algorithm 1 Basic Cluster-based Fact Finder (BCFF) (using TF) 1: Input: 1. Facts f provided by different providers related to ob2: 3: 4: 5:jects o O. 2. Implications matrix imp. Initialize to (p) to a value v w, where 0 v 1, o O where O is the set of all objects. while {o||tt - tt-1 | } do o o For every fact f , (f ) = log( pP (f ) (1 - to (p))) For every fact f , (f ) = (f ) + o(f )=o(f ) (f )imp(f f ) 1 For every fact f , s(f ) = (f )Clusters can be considered distinct if cluster conditional trust vectors are far apart from each other. Else, clustering is not really effective and hence our algorithms would not provide much gains. So, we perform smoothing and compute best fact = argmaxf ((1 - )sC (f ) + sG (f )) where sC and sG are cluster based and global fact confidence scores resp. was set to avg. cosine similarity between cluster centroids.5.EXPERIMENTS AND RESULTS6: 1+e 7: For every provider p, to (p) = s(f ) where f = fpo . 8: end while 9: Cluster to vectors using KMeans. 10: return s(f ) for every f , object clusters and topKworthy providers for each clustermost trust-This kind of clustering in the trust space is a novel form of clustering and may provide quite different clusters compared to clustering on natural dimensions. No training data or data related to any other dimensions of the objects are needed. But, note that there is no trust information sharing between objects in BCFF. Every iteration in Algorithm 1 simply recomputes trust of providers based on implications between various facts about the same object. Our algorithm should start with an initial clustering, perform trust analysis using this clustering (similar to BCFF) and then refine the clusters using the analysis obtained on the previous set of clusters. So, we define cluster-conditional trust, tck (p), as the trust of the provider p considering the facts published by the provider p related to objects in cluster ck . Algorithm 2 outlines our method which ensures the right information flow among related (in the sense of provider trust) objects only. It performs alternate clustering and trust analysis iterations. The clustering steps bring similar objects together, while the trust analysis steps compute better cluster-conditional trust rankings and better fact confidence values. While performing trust analysis, the flow of trustworthiness information for a provider is restricted to be within the current cluster. Modifications in fact confidence values (in analysis steps) leads to better object-conditional trust vectors which are then re-clustered to get modified clusters using KMeans clustering. Algorithm 2 Advanced Cluster-based Fact Finder (ACFF) 1: Input: 1. Facts f provided by different providers related to ob2: 3: 4: 5: 6: 7: 8:jects o O. 2. Implications matrix imp. Obtain clusters {ck }K BCFF k=1 while No change in clusters ck do Iteratively compute tc (p) for all clusters and providers and s(f ) for all facts. Compute to (p) for all objects using fact confidences s(f ). {ck }K KM eans(to ). Clustering is done on data of size k=1 (#clusters × #providers). end while return s(f ) and tk (p) for every f and p for every cluster ckWe experimented with multiple datasets: Abebooks.com Books Dataset (provided by Yin [4]), Wikipedia Biography Infobox Datasets and population dataset (subset of one used by Pasternack [2]). We cleaned all datasets to ensure that (provider, object) is a primary key. Original datasets somehow failed to maintain this constraint. In books (1265) datasets, facts are author lists, providers are book websites (894); ground truth (100 books) is obtained manually from book cover scans. In biography infobox dataset (258 people), facts are birth and death dates and providers are contributors (4392) on wikipedia; ground truth (24 bdates, 182 ddates) is obtained by consensus from multiple websites. For population data (30011 cities), fact is population of cities, providers are different wikipedia contributors (1361); ground truth (290 cities) is obtained from US Census data for 2000. We use two metrics: accuracy and compactness. Accuracy measures accuracy of most confident fact for any object obtained by different algorithms. Accuracy of a fact is defined differently for different types of facts (Books: same as [4], in population: 1 - dif f , dates: 1 - dif f 100 days ). Compactmax ness is used to evaluate clustering quality and is defined as intra-cluster similarity:avg. inter-cluster similarity. Table 1 shows accuracy gains of our cluster based fact finders over basic fact finders. For books dataset, clustering obtained does not seem to match with any natural clustering. For population dataset, with five clusters, we observe: contributors are clustered into (IL, CA, NY), (PA, VT, MI), (IL, AL, AR), (IN, IA, GA), (MN, OH, NY). Note we used cities for experiments, but report clusters on states for clarity.6. CONCLUSIONWe proposed algorithms for trust analysis using cluster based methods. We showed using four datasets that our algorithms perform better than traditional fact finders and generate interesting clusters. In the future, we plan to refine our clustering methods, e.g., by clustering in other spaces.Acknowledgements: Thanks to X. Yin and J. Pasternack for their datasets, and V. Vydiswaran and anonymous reviewers for comments. Research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-09-20053, DHS, and NSF IIS-09-05215. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government.7.54
21	information_diffusion social_media classification microblogs	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaPredicting Popular Messages in TwitterLiangjie Hong Ovidiu Dan Brian D. DavisonDept. of Computer Science and Engineering, Lehigh University Bethlehem, PA 18015 USA{lih307, ovd209, davison}@cse.lehigh.eduABSTRACTSocial network services have become a viable source of information for users. In Twitter, information deemed important by the community propagates through retweets. Studying the characteristics of such popular messages is important for a number of tasks, such as breaking news detection, personalized message recommendation, viral marketing and others. This paper investigates the problem of predicting the popularity of messages as measured by the number of future retweets and sheds some light on what kinds of factors influence information propagation in Twitter. We formulate the task into a classification problem and study two of its variants by investigating a wide spectrum of features based on the content of the messages, temporal information, metadata of messages and users, as well as structural properties of the users' social graph on a large scale dataset. We show that our method can successfully predict messages which will attract thousands of retweets with good performance.Table 1: Sample tweets with high retweetsRT @paramore Watch the World Premiere of Paramore's new video for 'Brick By Boring Brick' #paramore RT @CamaroWRX: http://bit.ly/794Edz because everyone #needsmorebradley RT @narendra: Please RT. Some recent thoughts on the empathic web. that made the Huffington Post - http://bit.ly/9WyxnTCategories and Subject DescriptorsH.3.3 [Information Storage and Retrieval]: Information Search and RetrievalGeneral TermsAlgorithms, Design, Experimentationaugment their social network. Both problems require us to determine the importance of messages. In this work we use "retweets" as a measure of popularity and address the problem by utilizing machine learning techniques to predict whether and how often new messages will be retweeted in the future. We treat the problem as a classification task. First, we train a binary classifier with positive and negative examples of messages which will be retweeted in the future. Second, we train a multi-class classifier which predicts the volume range of future retweets for a new message. To build the classifiers, we investigate a wide spectrum of features to determine which ones can be successfully used as predictors of popularity, including the content and topical information of messages, graph structural properties of users, temporal dynamics of retweet chains and meta-information of users. We conduct our experiments on a large scale dataset. The results suggest that we can successfully predict whether a message will be retweeted or not. Furthermore, we can also predict the volume of retweets with good predictive performance. Similar work by Suh et al. [2] studied a variety of factors that might influence retweets without explicitly showing their effectiveness in a classification framework.KeywordsInformation Diffusion, Social Media, Classification, Microblogs2. PROBLEMS & FEATURESWe cast the problem of predicting the popularity of messages into two classification problems: 1) a binary classification problem that predicts whether or not a message will be retweeted, and, 2) a multi-class classification problem that predicts the volume of retweets a particular message will receive in the near future. Two messages are considered identical if they share the same MD5 value. We sort all such messages by ascending time order, forming a chain of messages. The first message in the chain is the earliest version of the message in our dataset and all later messages in the same chain should contain at least one "RT" term. For the binary classification problem, for n messages in a chain, the first n - 1 messages are considered as "positive instances" and the last one as a "negative instance". In addition, all other messages which are not in any chains are also considered as "negative instances". For the multi-class classification problem, the number of messages following a specific message is treated as the number of retweets this message will receive. Therefore, for all messages, we assign a non-negative integer value to represent the number of subsequent retweets. It is difficult to predict the exact number of retweets a1.INTRODUCTIONSocial network services such as Facebook, Myspace and Twitter have become important communication tools for many online users. Such websites are increasingly used for communicating breaking news, eyewitness accounts and organizing groups of people. Users of these services have become accustomed to receiving timely updates on important events, both of personal and global importance. However, the flow of information within the social graph can lead to two problems. First, even users with few friends can experience information overload due to the high volume of messages. Second, users with a small number of social connections might miss important messages which do not reach them. In this second situation it would be useful to recommend interesting messages to such users, which might lead them to follow new users andCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.57WWW 2011 ­ Poster106March 28­April 1, 2011, Hyderabad, India0: Not retweeted, 1: Retweeted less than 100, 2: Retweeted less than 10000, 3: Retweeted more than 10000, similarly to [1]. We binned messages with retweets (size equals 10) and plot the distribution in Figure 1, showing that a large number of messages do not receive any significant retweets (less than 10) and a significant amount of messages also have more than 10000 retweets. Three messages with most retweets in Class 3 are shown in Table 1 as examples where the first one is about entertainment, the second is related to hockey and the third one is related to IT. We use Logistic Regression as the classifier, as we found it to be the most stable in preliminary experiments. We train the classifier on one week and test it on the next week and the results are the averages over four such experiments. We compare the results to two obvious baselines. One is based on whether a messages has been retweeted or not in the past, and the other uses the TF-IDF scores for terms in each message. The results are shown in the upper part of Table 2. For both baselines, the F1 score is roughly around 0.27, with relatively low Precision and Recall scores. For our method, the best performance is achieved by using "TF-IDF", "LDA", "degree distribution", "Retweet Before" and "User Retweet" features while all measures are much better than both baselines. In order to investigate the effectiveness of each feature, we remove individual features and see how the performance is affected. This is shown in the middle of Table 2, where the first column shows which feature have been taken away. We observe that the performance drops significantly when "degree distribution" and "Retweet Before" are removed, implying that they contribute greatly to the overall performance. We conjecture that in Twitter, users mainly see messages from their first level friends whom they are directly following. Therefore, the likelihood that some of a user's followers will retweet a new message depends on how many followers the user has. Thus, the effect of in-degree is obvious to some extent. For out-degree, since many messages which will be retweeted in the future have already been retweeted, a user with limited friends may face difficulties in seeing such messages. For multi-class classification, the lower part of Table 2 shows the performance achieved by using the best features in the binary classification. The results are extremely good in Class 0 and Class 3 but poor in Class 1 and Class 2, indicating that messages only attracting a small audience might be very different from the messages which receive huge numbers of retweets. After adding "Temporal" features, shown in the third column, the performance on Class 0 still holds but improvements in Class 1 and Class 2 are observed with some decrease of performance of Class 3. One explanation is that unlike popular messages which receive tens of thousands of retweets, normal messages only attract a small audience and users lose interest in them very quickly. Therefore, temporal features have a stronger effect on these messages with low and medium volume of retweets, compared to highly popular messages. User activity features can further improve the performance marginally, shown in the fourth column in Table 2.# Messages1041021000101001,000 # Retweets10,000100,000 550,000Figure 1: Retweet distribution Table 2: The performance of two classification tasksMethods Retweet Before TF-IDF Our Method Without User Retweet Without Degree Distr. Without Retweet Before Class 0 1 2 3 Precision 0.685 0.310 0.993 0.993 0.473 0.678 Previous Method Accu. 0.9999 0.0338 0.0522 0.9896 Recall 0.166 0.249 0.435 0.399 0.470 0.250 Temporal Accu. 0.9999 0.1139 0.4309 0.9004 F1 0.263 0.276 0.603 0.569 0.471 0.364 User Activities Accu. 0.9999 0.1490 0.4346 0.9209particular message will receive due to the fact that the maximum volume of retweets in the test set may be much higher than in its training set. Therefore, rather than directly predicting these integer values, we relax the problem by defining several categories to represent the volume of retweets and predict these categories instead. We investigate a wide range of possible features including content features, graph topological features, temporal features and metadata features, all of which are applicable to the two classification tasks. For content features, we use TF-IDF scores as a baseline and utilize Latent Dirichlet Allocation (LDA) to obtain the topic distributions for each message. For topological features, multiple popular features are considered, including global and local structures, such as PageRank, degree distribution, local clustering coefficient and reciprocal links. We also assume that users may track hot topics and may quickly switch topics over time. Therefore, for temporal features, we measure the time difference between the current message and the origin within the chain, the time difference between the current and the previous tweet, the average time difference of consecutive messages in the same chain and the average time a user's messages get retweeted. For meta information features, we are mainly interested in whether a message has been retweeted before (Retweet Before) and how many times in the past the messages generated by a particular user have been retweeted (User Retweet). Other user activities such as the total number of messages a user produced are also considered.AcknowledgmentsThis material is based in part upon work supported by the NSF under Grant Number IIS-0545875.3.EXPERIMENTSWe run our experiments with messages collected in November and December 2009, as well as the immediate social graph of the users which are active in this time period. The dataset contains 10,612,601 messages and 2,541,178 users. For the binary classification problem, we report Precision, Recall and F1 score. For the multi-class classification problem, we report accuracy for each class. Instead of directly predicting the exact number of retweets, we divide the messages into different retweet volume "classes":58
22	click_model learning_to_rank	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaAutomatically Generating Labels Based on Unified Click ModelGuichun Hua, Min Zhang, Yiqun Liu, Shaoping Ma, Liyun RuState Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology Department of Computer Science and Technology, Tsinghua University, Beijing 100084, Chinahuaguichun@gmail.com, {z-m, yiqunliu, msp}@tsinghua.edu.cn, lyru@vip.sohu.com ABSTRACTGround truth labels are one of the most important parts in many test collections for information retrieval. Each label, depicting the relevance between a query-document pair, is usually judged by a human, and this process is timeconsuming and labor-intensive. Automatically Generating labels from click-through data has attracted increasing attention. In this paper, we propose a Unified Click Model to predict the multi-level labels, which aims at comprehensively considering the advantages of the Position Models and Cascade Models. Experiments show that the proposed click model outperforms the existing click models in predicting the multi-level labels, and could replace the labels judged by humans for test collections. are more useful when fewer URLs are clicked; otherwise Position Models perform better. The users have diverse intention requirements and the queries submitted by users could be navigational, informational and transactional [1, 7]. For navigational queries, users usually click a fewer URLs, otherwise for informational and transactional queries. In this circumstance, it is not reasonable to use a general click model to predict the relevance between a query-document pair. The main contributions of this paper are that we propose a Unified Click Model to predict the label between a querydocument pair comprehensively considering the advantages of Position Models and Cascade Models. The experimental results show that the labels predicted by the Unified Click Model could replace the labels judged by humans and performs better than the labels predicted by the Position Model or Cascade Model alone.Categories and Subject DescriptorsH.3.3 [Information Search and Retrieval]: Retrieval Model2.UNIFIED CLICK MODELGeneral TermsAlgorithms, Performance, Experimentation.KeywordsClick Model, Learning to Rank, Ranking SVMThe Unified Click Model contains two parts: Query Intention Identification and Click Models. The Query Intention Identification predicts the query intention, and for each type of queries, different Click Models are chosen to predict the relevance between a query-document pair. The Query Intention Identification is the bridge to connect different kinds of Click Models.2.1Query Intention Identification1. INTRODUCTIONMany test collections have been builded as benchmark datasets for researchers comparing the performance of the models, such as T REC test collections and LET OR [6]. Each label in these test collections, which dipicts the relevance between a query-document pair, is usually judged by a human, and this process is very time-consuming and laborintensive. Many models, automatically generating labels from click-through data, have been proposed which could be classified into two categories: Position Models and Cascade Models. Position Models, such as [9], assume that the users' examining and clicking a URL in the returning result depends only on the position of this URL. While Cascade Models, such as [3], consider that the URLs ranking above the clicked URL have effects on the clicked one. Cascade ModelsCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.The decision tree C4.5 algorithm is chosen as the query intention classifier with two useful features nCS(n Clicks Satisfied) and nRS(Top n Results Satisfied) which are proved to be very efficient in [8]. The nCS and nRS are defined as below. #(Session of q less than n clicks) #(Session of q)nCS(q) =nRS(q) =#(Session of q clicks only on top n results) #(Session of q)#(Session of q less than n clicks) means the number of sessions, involving the query q, in which every one has the number of clicks less than n. #(Session of q) means the number of sessions involving the query q. And #(Session of q clicks only on top n results) means the number of sessions, involving the query q, in which every one has clicks only on top n results.59WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, India2.2 Click ModelsThe Position Model [9], referred to as COEC (clicks over expected clicks), and the Cascade Model [2], referred as DBN (dynamic bayesian network click model), are chosen as the click models in this paper.0.7 0.6 0.5 0.4 0.3 0.2HIL UCM_COECUCM_ALL UCM_DBN2.3 Using Unified Click Model to Predict MultiLevel LabelsIn the application of the Unified Click Model, we use the C4.5 algorithm to learn the thresholds to map the relevance predicted by click models into discrete label values. In this way, the Unified Click Model used to predict multi-level labels is a decision tree with features of nCS, nRS, COEC, DBN .3. EXPERIMENT 3.1 Experiment SettingsThe experiment data(including click-through data, queries and documents) we use in this paper is all extracted from a Chinese business search engine (http://www.sogou.com). The labels, depicting the relevance between a query-document pair in the training set, are 5 levels from 0 to 4, and 0 means irrelevant and 4 means perfectly relevant, which are judged by professional annotators. The click-through data is chosen from March 23rd to March 30th , 2924 queries and 1000 documents for each query are sampled and 87 ranking features are extracted for each querydocument pair, such as P ageRank and BM 25. We use 4 strategies to generate labels in the training set when traning a ranking function with Learning to Rank method Ranking SV M [5, 4], and compare the performance of ranking function based on these 4 strategies: HIL means that labels are judged by humans; U CM ALL means that labels are predicted by the Unified Click Model with all 4 features in section 2.3; U CM COEC means with three features of nCS, nRS and COEC; U CM DBN means with three features of nCS, nRS, and DBN . Three measures of P @N , N DCG@N and M AP are chosen to evaluate the performance of ranking functions learned through Ranking SV M based on the training set, which is labeled by 4 strategies. Figure 1: Performance Comparison with 4 Strategies Generating Labels search engines and contains the information that whether the users like the ranking for the URLs and which results the users prefer. So the Unified Click Model takes advantage of crowd wisdom in the click-through data.4.CONCLUSIONSIn this paper, we propose a Unified Click Model to predict the labels depicting the relevance between a querydocuments pair. This click model could unify the advantages of Position Model and Cascade Model and the experimental results show that labels predicted by our model could replace the ones judged by humans and outperform the labels predicted by other click models.5.ACKNOWLEDGMENTSThis work is supported by Natural Science Foundation (60736044, 60903107) and Research Fund for the Doctoral Program of Higher Education of China (20090002120005).6.The comparison performance of the ranking functions learned through Ranking SV M based on HIL, U CM ALL, U CM COEC, and U CM DBN with the evaluation measures of P @1, 5, 10, N DCG@1, 5, 10 and M AP shows in Fig. 1. The experimental results show that U CM ALL is the best method. We conducted t-tests on the improvements in terms of N DCG@1 10, and the results show that the improvements of U CM ALL over HIL, U CM COEC and U CM DBN are statistically significant (p - value < 0.01). In three label-predicted methods, U CM ALL is the only one that outperforms the HIL, and U CM COEC performs better than U CM DBN . From the experimental results, the Unified Click Model performs best in predicting the label between a query-document pair, even outperforms the labels judged by humans. This might be the reason that the quantity of labels predicted by Unified Click Model is larger than ones judged by humans. Because the click-through data is very easy to obtain from[1] A. Broder. A taxonomy of web search. In SIGIR Forum, 2002. [2] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In WWW, pages 1­10, 2009. [3] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In WSDM, pages 87­94, 2008. [4] R. Herbrich and et.al. Large margin rank boundaries for ordinal regression. In Advances in Large Margin Classifiers, pages 115­132, 2000. [5] T. Joachims. Optimizing search engines using clickthrough data. In KDD 2002, pages 133­142. [6] T. Qin and et.al. Letor: A benchmark collection for research on learning to rank for information retrieval. In Information Retrieval Journal, 2010. [7] D. E. Rose and D. Levinson. Understanding user goals in web search. In WWW, 2004. [8] L. R. Yiqun Liu, Min Zhang and S. Ma. Automatic query type identification based on click through information. In AIRS, 2006. [9] V. Zhang and R. Jones. Comparing click logs and editorial labels for training query rewriting. In WWW 2007 Workshop: Query Log Analysis: Social And Technological Challenges.60
23	flash_memory caching inverted_index	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaAllocating Inverted Index into Flash Memory for Search EnginesBojun HuangMicrosoft Research Asia Beijing, ChinaZenglin XiaMicrosoft Research Asia Beijing, Chinabojhuang@microsoft.com Categories and Subject DescriptorsH.3.3 [Information Search and Retrieval]: Search processzlxia@microsoft.comhard disk caching, [1] and [4] modeled it as knapsack problem, and proposed the standard greedy algorithm to approximate the optimal solution. However, the distinct characteristics of Flash memory requires modifications on existing optimization model. Specifically, our analysis considers the problem from following aspects: Throughput constrained optimization - Throughput and latency are two major constraints for storage management. Traditional disk caching systems concern mostly about latency, this may be partially because the long access latency of hard disk tend to hurt the overall performance first, and partially because the bandwidth of hard disk is instable across different workloads. However, the latency requirement in single index server is less restrictive since we could reduce the latency by splitting index into more partitions and serving them simultaneously. Furthermore, the throughput of Flash read is relatively insensitive to workloads and can be considerably large in its high-performance setup [3]. Thus, we take throughput/bandwidth as the main constraint in our optimization model. Cache size oriented optimization - Traditional disk caching systems try to minimize cache hit rate given a fixed cache size. This paradigm makes sense for latency constrained optimization, because a "consistently good" caching policy within traditional framework implicitly minimizes the cache size for a given cache hit rate, and a required cache hit rate can be uniquely translated to a required end-to-end latency. Note that the underlying assumption of this constraint translation is that the access latency of any storage remains the same while its size changes. Unfortunately, similar assumption doesn't hold in throughput/bandwidth constrained optimization, because the bandwidth of Flash memory grows as Flash chips added. Consequently, we have to explicitly minimizes (maximizes) the size of DRAM (Flash memory) while ensuring the throughput in Flash memory to be sustainable. Finally, the problem can be modeled as, given a set of index terms T where each index term t T is associated with a bandwidth requirement Bt and a capacity requirement Ct , also given a capacity-bandwidth function of Flash memory Ff lash : R+ R+ which indicates the sustainable bandwidth provided by a specified amount of Flash memory, then the goal is to select a subset S T for Flash memory and solve the optimization problem arg maxSTGeneral TermsAlgorithms, ExperimentationKeywordsFlash memory, caching, inverted index1. INTRODUCTIONAlthough most large-scale web search engines adopt the standard DRAM-HDD storage hierarchy, the usage of hard disk is greatly limited by its long read latency. On the other hand, NAND Flash memory is 100x faster than hard disk and 10x cheaper than DRAM [2]. Therefore, it's possible to allocate a significant portion of DRAM data into Flash memory, so as to save money on storage. This paper considers the optimal policy that allocates the DRAM portion of inverted index into Flash memory as much as possible. Note that the original hard disk portion of index data is still left in hard disk in our scheme, which actually results in a three-layer storage hierarchy. To our best knowledge, we are the first to show that it's possible to get substantially better system performance for web index serving by trying some Flash-aware storage management approaches, rather than just plugging in a SSD and treating it as super hard disk. We limit our discussion in the static scenario, where posting lists are allocated atomically in either Flash memory or DRAM only when the index updates and no other data movement is performed at run time. The problem is very similar to static index caching/pruning [1] [4], except that the caching here is exclusive and the target storage is Flash memory. Note that previous work suggested that static policies work well for inverted index caching, compared with their dynamic counterparts [1].2.ANALYSISIn static index allocation problem, a subset of index terms is selected to stay in Flash memory until the index is reallocated. Following the traditional analysis framework forCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.Ct , subject totS tSBt < Ff lash (tSCt )(1) Although Ff lash depends on specific package setup of the61WWW 2011 ­ PosterFlash memory used, it is convenient to arrange a bunch of Flash chips in single board [3] and to uniformly distribute the access requests (e.g. by stripping), in order to achieve fully parallelized integrated bandwidth. In this case, the integrated bandwidth will increase by a fixed amount for each Flash chip added, and thus is proportional to its capacity. This means Ff lash can be seen as a linear function Ff lash (x) = x. Interestingly, it can be proved that in this C case the greedy algorithm based on individual ratio Bt pert forms at least as good as what it does as the standard heuristic in knapsack problem, by noticing that it is still an ILP problem with the constraint of ( Bt - Ct ) < 0tSMarch 28­April 1, 2011, Hyderabad, IndiaFigure 1: The customized board with 36 Flash chipsC/B100% 90%QTF/DFCapacityQTFAccumulative Bandwidth(2)80%70%60% 50% 40% 30%The proof is omitted due to space limitation.3.EXPERIMENTATIONWe compare our algorithm with the state-of-art hard disk caching policy, as well as two simplified criteria in the greedy framework. We use a query log containing 0.7 million user queries which are already filtered by query result caching, and an index file covering 7 million web pages, both of which are from real search engine. Dynamic index pruning and adaptive matching algorithms are employed, which means posting lists may be partially scanned for serving a query. To achieve the full potential of Flash memory, we implemented a customized board with 36 Flash chips which has integrated bandwidth of at most 1.8 GB/s (shown in Figure 1). Specifically, the four caching policies we examined are · C/B the Flash-aware policy discussed in last section, C which is based on the individual ratio Bt . Bt is estit mated from the run-time logs. The run-time overhead for logging is found to be ignorable. · QTF/DF is based on the ratio of term access frequency and document frequency. It's the "optimal" policy in hard disk caching [1] [4]. · Capacity is solely based on capacity requirement, which doesn't need any run-time logging. · QTF is solely based on term access frequency. Note that this policy is equivalent to C/B when posting lists are always completely processed. Figure 2 illustrates our model, as well as the results of different policies, in a more intuitive way. For each policy, the index terms are sorted according to its respective criterion, thus the accumulative capacity requirement and accumulative bandwidth requirement of index terms can be drawn as a curve. All index terms on the left side of the intersection point between the curve and Ff lash will be allocated into Flash memory, and those on the right side will be in DRAM. To maximize Flash size, the intersection point should go right as much as possible. C/B does the best job and put more than 95% of the index data into Flash memory for system with 1GB Flash chip array (85% and 75% for 2GB chip and 4GB chip respectively). QTF, as a previously sub-optimal policy [1], has a similar curve with C/B and gives comparable allocation result for system with 4GB Flash chips. However, for 1GB Flash chips, the DRAM size of QTF is roughly 100% larger than20%10% 0% 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Accumulative CapacityFigure 2: Accumulative bandwidth-capacity distributions of different allocation policies that of C/B, because the intersection point in this case falls into the most different parts of these two curves. QTF/DF results in a nearly straight line, implying that it almost acts like randomly picking index terms into Flash memory, and performs even worse at the starting end of the curve (due to stopwords actually). Because the curve of QTF/DF is consistently higher than all the Ff lash lines, almost all index terms have to be allocated into DRAM under this policy.4. CONCLUSIONExperimental evidence shows that the previously optimal policy for hard disk doesn't work for Flash memory. The Flash-aware optimization should explicitly maximize the Flash size, under the constraint of memory throughput. When the integrated bandwidth of Flash memory grows linearly, the classic greedy algorithm for knapsack is still "good", and in our experiments it is able to allocate at most 95% of index data into Flash memory. Meanwhile, a previously sub-optimal policy becomes close-to-optimal in certain situations.5.62
24	entity_extraction query_logs data_mining	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaDomain-Independent Entity Extraction from Web Search Query LogsAlpa JainYahoo! Labs Sunnyvale, CA, 94089Marco PennacchiottiYahoo! Labs Sunnyvale, CA, 94089alpa@yahoo-inc.com ABSTRACTQuery logs of a Web search engine have been increasingly used as a vital source for data mining. This paper presents a study on largescale domain-independent entity extraction from search query logs. We present a completely unsupervised method to extract entities by applying pattern-based heuristics and statistical measures. We compare against existing techniques that use Web documents as well as search logs, and show that we improve over the state of the art. We also provide an in-depth qualitative analysis outlining differences and commonalities between these methods.pennac@yahoo-inc.comStarting with raw search query logs, our approach performs three steps: (1) identify candidate entities from search logs, (2) select reliable entities from the candidates using confidence scores, (3) execute a subsumption filter to eliminate noise. Generating candidate entities: User queries are short and lack syntactic structure, thus impairing traditional extraction approaches based on contextual evidence and syntactic interpretation. We therefore rely on a simple observation: oftentimes users construct their search query by copy-pasting phrases from existing texts in a web pages, thus carrying over surface-level properties such as capitalization. We generate candidate entities as contiguous capitalized words from a user query: given a query Q = q1 q2 · · · qn , we define a candidate entity E = e1 e2 · · · em as the maximal sequence of words (i.e., alpha-numeric characters) in the query such that each word ei in the entity begins with an uppercase character. This method can be far from perfect: for instance, a small fraction of user search queries are entered using only uppercase characters. We then discard spurious entities by employing text-based evidence described next. Deriving confidence scores: We assign two confidence scores to a candidate entity E. The representation score captures the intuition that the case-sensitive representation observed for E in Q, should be a likely representation for E, as observed on a Web corpus. For example given the query `Galapagos Island vacations', we assign a high score to the candidate `Galapagos Island' because we can observe it oftentimes on the Web. On the contrary, given the query `DOor HANGING tips', we assign a low score to the candidate `DOor HANGING', as it is seldom observed in that representation. More formally, the score is computed as: |(E)| (1) rw (E) = P iO(E) |(i)| where |x| is the number of occurrences of a string x in the corpus, O(E) is the set of all occurrences of string E, (i) is a casesensitive representation of the string i. The standalone score is based on the observation that a candidate E should often occur in a standalone form among the query logs, in order to get the status of proper entity: sq (E) = |Q == E | |queries that contain E| (2)Categories and Subject DescriptorsI.2.6 [Artificial Intelligence]: Learning--knowledge acquisitionGeneral TermsAlgorithmsKeywordsentity extraction, query logs, data mining1.QUERY LOGS ENTITY EXTRACTIONEntity extraction is an important part of many Web-based applications [3], defined as the task of extracting entities of pre-defined classes (e.g., `Brad Pitt' for the class Actors). Typically, extraction tasks are run over "well-formed" documents such as news articles or web pages [1]. Recently, Pasca [6] proposed to extract entities from query-logs instead of a classical Web corpus, by using a semisupervised approach that inputs a pre-defined list of classes represented by a small set of hand-made seeds. Extracting entities from query logs instead of web corpora has several advantages, in view of query-log-based application [4]. Along this direction, we present a completely unsupervised (i.e. we do not need seeds) and domainindependent (i.e., we do not need pre-defined classes) extraction method over query logs. Our study is inspired by the open domain information extraction (OIE) framework, that has been recently implemented to extract entities from Web-scale corpora, using simple unsupervised techniques [2]. In the same spirit, our method is also unsupervised, by applying heuristics that specifically address OIE over query-logs. To our knowledge, ours is the first attempt to devise an algorithm specifically designed to achieve the following two goals at the same time: (a) extract entities from a query log; (b) extract entities in an open-domain fashion (OIE).Copyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.We retain candidate entities for which rw (E) r and sq (E) s (we experimentally set r = 0.1 and s = 0.2). Applying the Subsumption Filter: As a final step, we deal with boundary detection. Often, the score-based filters miss erroneous candidates that have substantial overlap with good candidates. For example `Barack Obama' and `Barack Obama Biography' are both often used in capitalized form and in standalone queries, but the63WWW 2011 ­ Posterlatter is not an entity. We then eliminate candidates that completely subsume another one (e.g. `Barack Obama Biography').March 28­April 1, 2011, Hyderabad, Indiamethod QL-Base QL-Conf QL-Full Web # instances 15,077,998 2,391,201 2,067,385 2,388,412 accuracy 0.359 ±0.047 0.640 ±0.047 0.705 ±0.044 0.499 ±0.0492.EXPERIMENTAL EVALUATIONCompared methods: We experiment over a sample of 100M queries from the Yahoo! search engine in the first three months of 2009 (JN, FB, MR). We compare the following systems over MR (JN and FB are for training when needed): QL-Base: A baseline query log system, applying only the candidate generation step described in Section 1. QL-Conf: A system, applying candidate generation and confidence score filtering. QL-Full: Our full query log extraction method. QL-Pasca: The state-of-the-art query logs Pasca's system [6], bootstrapped as in [6] with 5 seeds per category (pattern extraction performed on JN, FB and MR, instance extraction and ranking on MR); we computed coverage at rank 20,000 and at full rank. Web: The state-of-the-art Web-based open-domain entity extraction system described in [5]; as corpus we use 500 million web pages crawled by the Yahoo! search engine; this system allows us to compare Web-based and query-log-based extraction. Evaluation method: We perform two experiments. In the accuracy experiment we draw a uniform random sample of 400 entities for each method, and ask two expert annotators if an entity is correct or not (Kappa agreeement over a shared set of 50, is kappa = 0.75). In the coverage experiment, we estimate a method's coverage over a gold set extracted from Wikipedia, for five entity classes 1 : actors (ACT), athletes (ATH), cities (CIT), diseases (DIS), and movies (MOV). Evaluation metrics: We use accuracy: the fraction of correct entities returned by a method, computed as |Sc | , where Sc is the set |S| of correct entities among the whole set S, and; coverage: the fraction of entities in the Wikipedia gold set G that are extracted by a method, as |GS| . |G|Table 1: Number of instances extracted by each system and related accuracy.method QL-Base QL-Conf QL-Full QL-Pasca WebACT ATH CIT DIS MOV72.75 64.16 63.02 10.51 65.3752.48 39.33 38.92 2.12 55.6570.17 14.63 14.46 18.73 68.0940.45 23.78 22.60 4.00 35.2057.13 28.61 26.43 20.01 37.46Table 2: Coverage of systems over the set of Wikipedia classes. talized on the Web, which are not entities (e.g. `Positive Test Results'). Overall, results suggest query logs as a promising source for extracting entities with high accuracy, in an open-domain fashion. Indeed, some effective techniques peculiar to query logs can not be used on the web, such as the standalone score. Coverage Experiment: Table 2 reports coverage for the target classes. As expected, of the various extraction methods involving query logs, QL-Base, shows the highest coverage, but at the expense of hurting the accuracy as discussed above. In comparison with the Web, our methods that use filters, QL-Conf and QL-Full, show lower coverage due to the aggressive round of the applied filters, especially the standalone ratio: in case of instances from CIT, this may prove to be too restrictive as users tend to query for information about a city, (e.g., `Rome attractions' or `Shanghai map') as opposed to querying for the city (e.g., `Rome' or `Shanghai'). Extending our confidence score functions to incorporate other evidence available in query logs to relax this effect is part of future work. As a final remark, our methods outperform QL-Pasca for all but the CIT class for the reasons discussed above. It is noteworthy that none of the systems achieve full coverage over Wikipedia, which does not mean that Wikipedia is an exhaustive source of entities. Indeed, there are many correct instances extracted by the experimented systems that are not present in Wikipedia. To support this, we sampled and evaluated 100 entities extracted by QL-Full which are not in Wikipedia. We observed that 61% of these instances are correct, i.e. Wikipedia misses a large number of entities. In conclusion, we presented a completely unsupervised method to extract entities from query logs, that improves over the state of the art. This paper takes an initial step towards building OIE over search query logs, opening ample space for promising future work on information extraction, and for building domain-independent applications based on query logs, such as tools for intent modeling and query suggestion.2.1Experimental ResultsAccuracy Experiment: Table 1 reports the accuracy as well as the total number of entities extracted for all open-domain extraction techniques. Note that results for QL-Pasca cannot be reported, as it is not an open-domain system, hence numbers would not be comparable. As for the query log based systems, results show that applying filters to the raw entities, largely improves the accuracy at the cost of reducing the total number of entities. As we will see in the coverage experiment, the decrease in the total number of entities reduces the coverage by only a small margin, indicating that the filters correctly discard erroneous candidates. The application of the confidence scores is highly effective: QL-Conf improves +23% points over the baseline QL-Base. Candidates such as `Buy' and `News' are discarded for their low representation score, while `HP Printer Software Free Download' and `About Israel' are filtered using the standalone score. The subsumption filter further improves the accuracy (QL-Full improves +6.5% over QL-Conf) showing that the confidence scores alone cannot do the whole job. For example, `Chicago White Sox Tickets' passes the confidence filters, but is discarded, since it subsumes `Chicago White Sox'. The Web system performs cosiderably worse than the query log systems. Typical errors include: candidates where the name of a person is augmented with his title (e.g. `Professor Iam Wilut'); tokenization errors (e.g.`surely­') ; noun phrase commonly capi1 An exhaustive generic set of entities is not available in the literature and is impractical to build.3.[1] S. Chaudhuri, V. Ganti, and D. Xin. Exploiting web search to generate synonyms for entities. In WWW-09, 2009. [2] D. Downey, M. Broadhead, and O. Etzioni. Locating complex named entities in web text. In IJCAI, 2007. [3] J. Hu, G. Wang, F. Lochovsky, J. tao Sun, and Z. Chen. Understanding user's query intent with Wikipedia. In WWW-09, 2009. [4] A. Jain and M. Pennacchiotti. Open entity extraction fromweb search query logs. In COLING-2010, 2010. [5] P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu, and V. Vyas. Web-scale distributional similarity and entity set expansion. In EMNLP-09, 2009. [6] M. Pasca. Weakly-supervised discovery of named entities using web search queries. In CIKM-2007, 2007.
25	recommender_systems collaborative_filtering learning_to_rank context	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaRanking in Context-Aware Recommender SystemsMinsuk Kahng, Sangkeun Lee, Sang-goo LeeSchool of Computer Science and Engineering Seoul National University Seoul, South Korea{minsuk,liza183,sglee}@europa.snu.ac.krABSTRACTAs context is acknowledged as an important factor that can affect users' preferences, many researchers have worked on improving the quality of recommender systems by utilizing users' context. However, incorporating context into recommender systems is not a simple task in that context can influence users' item preferences in various ways depending on the application. In this paper, we propose a novel method for context-aware recommendation, which incorporates several features into the ranking model. By decomposing a query, we propose several types of ranking features that reflect various contextual effects. In addition, we present a retrieval model for using these features, and adopt a learning to rank framework for combining proposed features. We evaluate our approach on two real-world datasets, and the experimental results show that our approach outperforms several baseline methods.Categories and Subject DescriptorsH.3.3 [Information Search and Retrieval]: Information filtering, Retrieval modelsGeneral TermsAlgorithms, Experimentation(e.g. weather) can affect the preferences independent of the given user in some applications. On the other hand, there can be user-dependent contextual effects [5]. Furthermore, a set of contextual variables can be more important than any one individual contextual variable [8]. Other varied forms of contextual effects can be possible in the real world. To tackle the problem, we propose a novel method for context-aware recommendation, which incorporates several features into the ranking model. We are motivated by the ranking process in search engines, which employs various features [6]. We first formulate the problem of context-aware recommendation as searching the most suitable items with respect to a given user and his context. By decomposing a query, we propose five types of features that reflect various contextual effects. We present a retrieval model for using these features [7], and adopt a ranking framework called learning to rank [6, 3] for combining proposed features. Our approach has two major advantages. First, service providers can improve the quality of recommendation by simply adding appropriate features into the ranking model. With the help of learning to rank, the system can construct an optimal function for the application. In addition, it supports flexibility in that service providers can choose features suitable for their specific purposes.2.PROBLEM FORMULATIONKeywordscontext-aware recommender systems, recommender systems, collaborative filtering, learning to rank, ranking in information retrieval, context, usage log1.INTRODUCTIONThe goal of recommender systems is to estimate a user's preference and deliver a list of items that might be preferred by the given user. As it is recognized that preferences can be affected by the user's context, context-aware recommender systems aim to provide recommendation of better quality by utilizing available contextual information (e.g. time, location) of the user [1, 4]. However, incorporating context into recommender systems is not a simple task in that contextual effects on the users' preferences can be diverse depending on the application and domain. For example, some contextual variablesCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.The user's behavior, such as accessing a webpage and listening to a song, is stored in an item usage log. An item usage log has a set of events E = {E1 , . . . , En }. An event Ei consists of user ui , context ci , and item di . We define context as a composition of contextual values vit , where vit is an instance of t-th contextual variable. Table 1 shows an example of an item usage log. We define the problem of context-aware recommendation as follows: Given a query q, which consists of a user u and his current context c, the system ranks all candidate items d D, and recommends the top-k suitable items. In other words, for each candidate item d, p(d|u, c) is calculated, and then the system produces a list of k items whose probabilities are the highest. Table 1: Example of Item Usage Log User Context Item Username Time of Day Location Weather Song ID bluesky morning office sunny 340 bobsmith evening coffee shop rainy 370 musiclover evening home cloudy 37065WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, India3. 3.1RANKING ITEMS USING CONTEXT Ranking Features4.EXPERIMENTSWe propose five types of features that reflect various contextual effects. To obtain them, we decompose a query that consists of a user u and a set of contextual values v. The first type of feature does not consider any component of the query; items are recommended based on their global popularity. The probability of an item d given a user u and context c can be written as: p(d|u, c) p(d). The second type of feature, similar to the traditional collaborative filtering [2], considers the user for recommending items. The equation becomes: p(d|u, c) p(d|u). The third type of feature considers one individual contextual variable. Preferences are dependent on individual contextual values regardless of the user as shown below. For example, some song items are preferred more on rainy days. p(d|u, c) p(d|v). The fourth type of feature pairs up two of the components: the user and any one contextual variable. It models userdependent contextual effects [1, 5]. For instance, some users like certain songs in the morning, while others do not. p(d|u, c) p(d|u, v). The last type of feature considers all components of the query, which can be thought of as abstracted context [8]. p(d|u, c) p(d|u, v1 , . . . , vT ), where T is the number of contextual values.We evaluate our method on two real-world datasets. First, we use a music listening log gathered from the music streaming service called Bugs (http://www.bugs.co.kr). We incorporate all five types of features into the model using user and three contextual variables: date, time of day, and weather. Second, we collect a place `check-in' log from the location-based service named Foursquare (http://foursquare. com). We choose user, GPS location, date, and time of day. The system suggests the top-k places for a query. We use normalized discounted cumulative gain (NDCG) as an evaluation measure. We consider each event as a different query, and check if the model produces a list that includes the item in the event [5]. We compare our method to three baseline methods: popularity (type 1), collaborative filtering using LDA (type 2), and reduction-based approach [1]. Table 2 shows a comparison of our method to the baseline methods on two datasets. Our method which uses Ranking SVM outperforms all baseline methods. This is because our approach, compared to other methods, can incorporate various types of features into the unified ranking model. Table 2: NDCG@k comparison on two datasets Music Foursquare Method @5 @10 @5 @10 Popularity 0.0670 0.0902 0.0199 0.0246 User only (CF) 0.0687 0.0946 0.0822 0.0996 0.0909 0.1030 0.2368 0.2408 Reduction Ranking SVM 0.2161 0.2359 0.4612 0.48905.CONCLUSION3.2Retrieval ModelWe adopt the existing LDA-based document retrieval model proposed by Wei and Croft [7]. Based on the query likelihood model they used, we assume p(d|u, c) p(u, c|d). For the fourth type of feature, the model can be written as: p(u, v|d) = Nd Nd pM L (u, v|d) + (1 - )pM L (u, v|C) Nd + µ Nd + µ + (1 - )pLDA (u, v|d),In this paper, we propose a novel method for contextaware recommendation by incorporating several features into the ranking model. We propose five types of features that model various contextual effects. We present a retrieval model for using these features, and utilize the learning to rank framework for combining the features. The experimental results show that our approach performs better than some baseline methods.6.where pM L is for the maximum likelihood estimation, `C' represents the whole item collection, and pLDA is for the LDA. The LDA topic model finds similar users or context by extracting the latent topics z from the given data: pLDA (u, v|d) = p(u, v|d , ) =kp(u, v|zk , )p(zk |d ).3.3Learning to Rank ItemsOnce we have several features and the retrieval model, we need a ranking function to produce a ranked list of items. Using a machine learned ranking framework called learning to rank [6], we obtain a ranked result by combining the features. We choose to use Ranking SVM, one of the well known learning to rank algorithms. To apply this pairwise approach, we randomly generate some negative feedback since we have only positive feedback, and we assume that the item selected by the user is more preferred than any other item.[1] G. Adomavicius and A. Tuzhilin. Context-aware recommender systems. In Recommender Systems Handbook. Springer, 2010. [2] A. S. Das, M. Datar, A. Garg, and S. Rajaram. Google news personalization: scalable online collaborative filtering. In WWW '07. ACM, 2007. [3] F. Diaz, D. Metzler, and S. Amer-Yahia. Relevance and ranking in online dating systems. In SIGIR '10. ACM, 2010. [4] A. Karatzoglou, X. Amatriain, L. Baltrunas, and N. Oliver. Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering. In RecSys '10. ACM, 2010. [5] D. Lee, S. E. Park, M. Kahng, S. Lee, and S.-g. Lee. Exploiting contextual information from event logs for personalized recommendation. In Computer and Information Science 2010. Springer, 2010. [6] T.-Y. Liu. Learning to rank for information retrieval. Found. Trends Inf. Retr., 3:225­331, 2009. [7] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In SIGIR '06. ACM, 2006. [8] E. Zheleva, J. Guiver, E. Mendes Rodrigues, and N. Mili´-Frayling. Statistical models of music-listening c sessions in social media. In WWW '10. ACM, 2010.
26	social_media latent_factor_models	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaSmart News Feeds for Social Networks Using Scalable Joint Latent Factor ModelsHimabindu Lakkaraju klakkara@in.ibm.comIBM Research - IndiaAngshu Rai angshu.rai@in.ibm.comIBM Research - IndiaSrujana Merugu srujanamerugu@in.ibm.comIBM Research - IndiaABSTRACTSocial networks such as Facebook and Twitter offer a huge opportunity to tap the collective wisdom (both published and yet to be published) of all the participating users in order to address the information needs of individual users in a highly contextualized fashion using rich user-specific information. Realizing this opportunity, however, requires addressing two key limitations of current social networks: (a) difficulty in discovering relevant content beyond the immediate neighborhood, (b) lack of support for information filtering based on semantics, content source and linkage. We propose a scalable framework for constructing smart news feeds based on predicting user-post relevance using multiple signals such as text content and attributes of users and posts, and various user-user, post-post and user-post relations (e.g. friend, comment, author relations). Our solution comprises of two steps where the first step ensures scalability by selecting a small set of user-post dyads with potentially interesting interactions using inverted feature indexes. The second step models the interactions associated with the selected dyads via a joint latent factor model, which assumes that the user/post content and relationships can be effectively captured by a common latent representation of the users and posts. Experiments on a Facebook dataset using the proposed model lead to improved precision/recall on relevant posts indicating potential for constructing superior quality news feeds.Categories and Subject DescriptorsH.3.3 [Information Storage and Retrieval]: Information Search and Retrievalfor both generic and personal information. However, it is currently non-trivial to discover all the relevant information or sources in a social network beyond the immediate social graph [4]. The key technical challenge is to design scalable techniques that can combine a large variety of sparse, high dimensional signals, such as text content and attributes of posts and users, and dyadic user-user, post-post, user-post relations (e.g. network linkage, authorship, commenting activity) to predict other relationships of interest, e.g. the relevance of a post to a user or to another post. Currently, there exists related work in the area of personalized news recommendation [3] and social network-based search [5] where the relevance of a post to a user is modeled in terms of the structured user-post attributes and user-user (activity or linkage) correlation. Of these, the techniques based on discriminative models, require substantial feature engineering effort in addition to handling missing observations, while those based on generative models are not very scalable and handle a single dyadic relation. In this work, we consider the problem of constructing a smart news feed by modeling user-post relevance. The novelty of our approach lies in (i) ensuring scalability of the generative model for user-post interactions by conditioning it on a selection variable, which can be computed fast using inverted feature indexes in a prior step, and (ii) combining the predictive power of multiple dyadic relations and text content using block and topic models coupled using a common latent representation for the users and posts.2. PROBLEM STATEMENTLet U denote the set of users, P, the set of posts. For each user u U, let cU and xu denote the text content u and demographic attributes (e.g. gender) in the user profile. Similarly, for each post p P, let cP and yp denote the text p content and structured attributes, (e.g. hasLink). Further, for each dyad of users (u, v) U × U , let rU U denote a u,v vector encoding various relationships between the user dyad (u, v), e.g. friend, follower, etc. Similarly, let rP P and rU P p,q u,p denote encoding of relationships between the dyads (p, q) P × P and (u, p) U × P. Given observations on user and post-specific properties, and (possibly incomplete) useruser, post-post, user-post relationships, the goal is to predict user-post relevance (or in general, some fine-grained userpost interaction), in order to obtain all the relevant posts for each user.General TermsAlgorithms, ExperimentationKeywordssocial media, news feeds, latent factor models1.INTRODUCTIONWith the ever-increasing participation of authoritative news sources and the availability of flexible communication protocols social networks such as Facebook/Twitter have become the dominant acquisition and dissemination systems Amore detailed version of this work is available online https://researcher.ibm.com/researcher/view.php?person=in-klakkara atCopyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.3. SOLUTION APPROACHWe address the above problem using a two step approach. The first step involves selecting a small set of dyads with73WWW 2011 ­ Posterpotentially interesting interactions using inverted featurebased indexes. The second step assumes the first step selection variables to be fully observed, and models the dyadic interactions, post and user content using a joint latent factor model. Selection of Interesting Dyads. We choose a set of easy-to-compute, predictive features for the relevant dyad types (user-user, post-post, user-post). For instance, in the case of user-post dyads, these include network distance between user and post author, immediate neighborhood size of post author, etc. The observed data is preprocessed to create maps from (user, feature, feature-values) to matching posts. For each feature, a threshold is identified such that the feature-threshold predicate (e.g. feature value < threshold) provides high recall and good precision for userpost pairs with interesting interactions. These predicates are then combined disjunctively to yield the desired high filter. Joint Latent Factor Model. The joint model in the second step is based on a key assumption that the dyadic (useruser, user-post, post-post) interactions and user/post text content can be explained using a compact latent representation of users and posts along with the observed user/post attributes. This latent representation not only provides a principled way of integrating information from multiple dyadic relations and text content, but also helps in avoiding issues arising due to high dimensionality and sparsity. The latent representation for a user u and a post p takes the form of a mixed membership across multiple user and post clusters U P and denoted by u and p respectively. The generative process can be briefly summarized as follows:U P u Dir(U ), p Dir(P ), u U, p P, UU UU UP UC U zu,v S , zu,v T , zu,p , zu M ult(u ), u, v U , p P, PP PP PU P P zp,q S , zp,q T , zp,u , zp C M ult(p ), p, q P, u U, UU UU rU U BM U U ( UU , zu,v S , zv,u T , UU , xu , xv ), u, v U, u,v PP PP rP P BM P P ( P P , zp,q S , zq,p T , P P , yp , yq ), p, q P, p,q UP PU rU P BM UP ( U P , zu,p , zp,u , UP , xu , yp ), u U , p P, u,pMarch 28­April 1, 2011, Hyderabad, IndiaBaseline Precision Relative Recall F-measure 0.072 1.0 0.1343 Joint Model 0.3659 0.8851 0.5178 Text Content 0.3078 0.7331 0.4334 Structured Attributes 0.1915 0.4726 0.2725Table 1: User-post relevance prediction (3 fold CV). "personal", "sentiment", "question" and specific instances of "concepts" can be readily defined in terms of word distributions while the other aspect choices tend to be defined by their complementary relation and hence, we learn a LDA model [2] with these topics seeded with a few exemplary words.4. EXPERIMENTAL RESULTSUsing a Facebook application that users can sign up for, we constructed a dataset comprising of user friendship linkage, user-post authorship, commenting activity, user profiles and detailed post content (text/attributes such as message type-status/video/link). We considered a subset of this data with 257 users, 2441 posts and 1158 known friendship links. To evaluate the proposed model, we assume that one or more comments by a user on a viewed post is a proxy for the user-post relevance that we seek to predict. Table 1 shows prediction quality for 4075 pairs of user and viewed posts (i.e., user is friend of post author) from the collected data using 3 fold cross validation for different approaches: baseline Facebook feed, proposed joint modeling approach, as well as separately using the different predictive signals. The models were trained with 6 user clusters, 7 post clusters, 7 topics, and inference algorithm run for 500 iterations. The results point to the efficacy of the joint latent factor model in filtering out irrelevant posts while retaining most of the relevant ones. We also performed an evaluation of smart feeds constructed from unseen posts (i.e., not authored by friends) using the joint model on a small set of users and obtained precision of 61.11% on average, indicating that the joint model enables discovery of new content that is not in the immediate neighborhood.cu T M ( , zu cP p T M (P PUCCUC, xu ), u U, p P.P , zp C , yp ),5. CONCLUSION AND FUTURE WORKPreliminary results using the joint latent factor model based approach for estimating user-post relevance are quite promising, but further experimentation with different cluster parameters, inference schemes and larger data sets needs to be performed. The dynamic nature of social network data also makes it critical to study incremental inference algorithms and temporal trends.Here, BM and T M refer to block models and topic models respectively. The variables U , P denote the hyperparameters corresponding to the cluster membership priors, · · while zu,· and zp,· denote realizations of user and post clusU P ter labels for different dyads, and zu C and zp C denote the ones that influence the user/post text content. The dyad specific cluster labels along with the block model parameters , the observed user and post attributes (xu , yp ), and attribute coefficients generate the dyadic interactions via generalized linear models (GLM) as in [1](e.g. logistic regression for binary interactions). Similarly, the cluster laU P bels zu C , zp C along with the topic model parameters and user/post attributes (xu , yp ) generate the user profile and post content using an LDA model [2]. Estimation of parameters and latent variables in the joint model is done using an approximate Gibbs sampling algorithm assuming default interactions for the dyads not selected in the first step. Aspect LDA Model for Posts. We observe that posts can be associated with few key aspects that are highly predictive of user-post interactions. These include: (a) scopepersonal/non-personal, (b) message type- sentiment/question/ statement, (c) concept- science/sports, etc. Of these,74
31	query_structure segmentation	WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaUnsupervised Query Segmentation Using only Query LogsNikita Mishra* Rishiraj Saha Roy* Niloy GangulyIndian Institute of Technology Kharagpur Kharagpur, India 721302Srivatsan LaxmanMonojit ChoudhuryMicrosoft Research Lab India Bangalore, India 560080nikitamishra07@gmail.com, {rishiraj, niloy}@cse.iitkgp.ernet.in ABSTRACTWe introduce an unsupervised query segmentation scheme that uses query logs as the only resource and can effectively capture the structural units in queries. We believe that Web search queries have a unique syntactic structure which is distinct from that of English or a bag-of-words model. The segments discovered by our scheme help understand this underlying grammatical structure. We apply a statistical model based on Hoeffding's Inequality to mine significant word n-grams from queries and subsequently use them for segmenting the queries. Evaluation against manually segmented queries shows that this technique can detect rare units that are missed by our Pointwise Mutual Information (PMI) baseline.{slaxman, monojitc}@microsoft.comPrevious research has expressed and addressed the need for identification of these units [1 - 5], a process termed as query segmentation. Nevertheless, efforts have been mainly directed towards identification of multiword named entities [1] and natural language phrases [2]. Towards this end, various external resources such as Webpages [3, 5], search result snippets [4] and Wikipedia titles [5] have been used. Although these methods can help in retrieval, query expansion and query suggestion, we strongly believe that they miss out on the unique syntactic properties of queries due to a bias towards projecting natural language structure on queries. Thus, we think that the linguistic structure of queries is distinct from that of the standard language (i.e., English, in our case); the first step towards understanding this structure is to understand the nature of the constituent word groups. These word groups should be identified solely on the basis of queries, because use of external resources raises the risk of projecting natural language structures onto the queries; and a proper understanding of this structure coupled with automatic techniques for parsing it can lead to significant performance improvements in various IR tasks. In this work, we take the first steps to unravel the structure of queries by proposing an unsupervised method for query segmentation that uses only query logs. As we shall see, the segments identified by our method do not necessarily align with natural language segments, yet it is clear that they are meaningful.Categories and Subject DescriptorsH.3.3 [Information Search and Retrieval]: Retrieval ModelsGeneral TermsAlgorithms, Measurement, ExperimentationKeywordsQuery Grammar, Query Structure, Segmentation, Hoeffding's Inequality Unsupervised Query1. INTRODUCTIONWeb search queries with length between 3 and 10 words, which constitute approximately 80% of all queries in the query log that we have analyzed here, seem to have a unique structure; they are neither bags-of-words, nor grammatically correct natural language phrases or sentences. For example, the queries 3g not working nokia n96 telstra australia and nokia n96 telstra australia 3g not working can both be paraphrased in natural language as "3G is not working in a Nokia N96 mobile phone bought from the Telstra store in Australia." The queries seem to have been derived from the underlying English sentence by dropping the stop words like is and from, stripping off the common nouns such as phone and store, and randomly permuting the left over chunks ­ 3g not working, nokia n96, and telstra Australia. This leads us to the interesting observation that queries are, in fact, bags-of-units, as opposed to bags-ofwords.* Part of this work has been done during the authors' internship at Microsoft Research India Copyright is held by the author/owner(s). WWW 2011, March 28­April 1, 2011, Hyderabad, India. ACM 978-1-4503-0637-9/11/03.2. METHODWe are given a large collection of search queries. Consider an nwhere wj-s denote the words gram constituting M. Let denote the subset of queries in the log that contain all the words of M, though not necessarily occurring together as an n-gram. Our premise is that search queries can be viewed as bags of Multi-Word Expressions (MWEs), which is to say that any permutation of the MWEs constituting a particular search query will effectively represent the same query. Thus, to test if an observed n-gram is an MWE, we could ask if the constituents of an MWE appear together more frequently than they would under a bag-of-words null model. We now formalize this intuition in a new test of significance for detecting MWEs in search queries. Let us fix our focus on M, a candidate MWE. Let be the indicator variable for the event "M occurs in the query ". Let denote the probability of this event and let be the length of . There are locations where M can be positioned in and for each choice of location there are ways of permuting the remaining non-MWE words of .91WWW 2011 ­ PosterMarch 28­April 1, 2011, Hyderabad, IndiaThus, we can write the probability of words model (null) as follows:­ ­] under the bag-of-... (1)We define (which models the number of times the words of M appear together in the k queries). We use Hoeffding's Inequality to obtain an upper-bound on the probability of , where N denotes the observed value of X in the data (also referred to as the frequency of M): ... 2 where, the expectation is given by . We obtain for each n-gram M and define as the MWE score for M. If is small, then the surprise factor is higher indicating a greater chance of M being an MWE, and vice versa. We note that unigrams have a score of zero, since their observed and expected frequencies are equal. For computational reasons, we compute the MWE scores only for n-grams whose constituent words have each appeared in at least queries in the database (where is a user-defined threshold). We add an n-gram to the list of significant n-grams if its MWE score exceeds (a second user-defined threshold). In our experiments we used and (where k is the number of queries in which all the words of the n-gram occur, though not necessarily together). We now have a list of significant n-grams and their associated MWE scores. We use this list to perform unsupervised query segmentation as follows: First, we compute a final score for each possible segmentation by adding the MWE scores of individual segments. Then we pick the segmentation that yields the highest segmentation score. Here we use a dynamic programming approach to search over all possible segmentations.method. While a fake bill is a noun phrase, and therefore, a valid segment according to the Standard English grammar, one cannot deny the fact that how to expresses a class of intent in queries and is found to be associated with diverse concepts such as save money, play guitar or make tea. Interestingly, spot a fake, which makes very little sense as an MWE, is in fact quite commonly seen in queries expressing a generic action phrase applicable to diverse objects such as video, gucci bag or mona lisa painting. Some other examples of generic query intents discovered by this method are information about, difference between and history of the. The proposed solution is also capable of detecting named entities such as windows media player and nikon d5000, including rare ones like very hungry caterpillar. The disagreements between the segmentation by this method and manually annotated data are partly due to influence of English grammar on annotators and inherent ambiguities in some queries, and partly due to lack of domain knowledge which makes it hard to judge the statistical significance of rarer named entities and multiword expressions. The latter can be suitably addressed by using external resources such as Wikipedia, though adequate care has to be taken so that the generic intent phrases are not lost in the process. The accuracy figures reported here are lower than the state-of-the-art, but it should be emphasized that since we do not use any external resources or manually segmented data to learn the models, our results are not comparable to those reported earlier. Moreover, the motivation and goals of our work are fundamentally different.4. CONCLUSIONS AND FUTURE WORKIn this work, we have proposed an unsupervised method of query segmentation that uses Web queries as the only resource. The method unravels structural units of queries that are distinct from natural language phrases and outperforms the PMI baseline in every metric. Currently we are enriching the segmentation scheme by using lists of named entities obtained from other sources and conducting linguistic and statistical analysis of the segmented queries to discover deeper structural patterns.3. EVALUATIONAll our experiments have been performed on a subset of one million queries (from a total of 342 million) collected through Bing Australia (http://www.bing.com.au). The segmentation accuracy was evaluated using four standard metrics discussed in [5] against a manually segmented set of one thousand six-word queries (handling upto 5-grams). The PMI threshold for MWE significance is 8.2. Results are shown in Table 1. Table 1. Segmentation Accuracies (in %) Method PMI Proposed Scheme Seg-Acc 70.69 75.20 Precision 49.23 54.95 Recall 54.59 60.09 F-score 51.77 57.415. ACKNOWLEDGMENTSWe would like to thank Bhaskar Mitra, Anjana Das and Victor Das from Bing, India for providing us with the data.The results show that our scheme performs better than a baseline method that uses PMI. On close examination of the segmentation results, we found that many segments discovered by our scheme did not match with human annotations because human segmentation is largely influenced by natural language grammar. For example, the query (how to spot) (a fake bill), where parentheses mark the segmentation boundaries by manual annotators, is segmented as (how to) (spot a fake) (bill) by our92
